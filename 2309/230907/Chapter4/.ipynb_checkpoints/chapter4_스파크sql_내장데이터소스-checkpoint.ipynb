{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "405995eb-37ae-49cf-bc93-10ace29bd19d",
   "metadata": {},
   "source": [
    "# 스파크 애플리케이션에서 스파크 SQL 사용하기\n",
    "SQL 쿼리를 실행하기 위해선 spark라고 선언된 SparkSession 인스턴스에서 spark.sql('SELECT * FROM myTableName')과 같은 sql() 함수를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afd2d37-1988-4710-aed8-0d8460c7161c",
   "metadata": {},
   "source": [
    "## 기본 쿼리 예제\n",
    "스키마를 사용하여 제공된 데이터를 데이터 프레임으로 읽고, 데이터 프레임을 임시 뷰로 등록하여 SQL로 쿼리할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6339a3a-82ee-4d62-a688-6ebf462a051e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 14:25:22 WARN Utils: Your hostname, minseok-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/09/07 14:25:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/07 14:25:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName('SparkSQLExampleApp')\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f91e731-817f-4f6c-aeaf-05bb8dffcbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 데이터 세트 경로\n",
    "csv_file = 'departuredelays.csv'\n",
    "\n",
    "# 읽고 임시뷰를 생성\n",
    "# 더 큰 파일의 경우 스키마를 지정해주도록 하자\n",
    "'''스키마를 지정하고 싶다면\n",
    "schema = \"'date' STRING, 'delay' INT, 'distance' INT, 'origin' STRING, 'destination' STRING\"\n",
    "책에 나온대로 정확히 적었는데 에러발생\n",
    "컬럼명에 따옴표를 제거하고 실행하면 에러가 발생하지 않았음\n",
    "schema = \"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\n",
    "'''\n",
    "# df = (spark.read.csv(csv_file, schema=schema))\n",
    "df = (spark.read.format('csv')\n",
    "      .option('inferSchema', 'true')\n",
    "      .option('header', 'true')\n",
    "      .load(csv_file))\n",
    "df.createOrReplaceTempView('us_delay_flights_tbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88438fb3-de5e-4db6-b490-ce4e2767521d",
   "metadata": {},
   "source": [
    "임시뷰를 사용할 수 있고, 스파크 SQL을 사용하여 SQL쿼리를 실행할 수 있다. <br>\n",
    "스파크 SQL은 ANSI:2003과 호환되는 SQL 인터페이스를 제공한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165bd721-ffa3-47bd-acbf-3a56fbbc60d0",
   "metadata": {},
   "source": [
    "미국 항공편 운항 지연 데이터세트\n",
    "- date 칼럼은 02-19 09:25 AM으로 매핑되는 02190925와 같은 문자열을 포함하고 있다.\n",
    "- delay 칼럼은 계획된 도착시간과 실제 도착시간의 차이를 분으로 제공한다. 조기 도착의 경우 음수로 표기된다\n",
    "- distance(비행거리) 칼럼은 마일 단위로 출발지와 도착지의 거리를 제공한다.\n",
    "- origin(출발지) 칼럼은 출발지의 IATA 공항 코드가 포함된다.\n",
    "- destination(도착지) 칼럼은 도착지의 IATA 공항 코드가 포함된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02da856-df04-43e5-9076-02022ff85962",
   "metadata": {},
   "source": [
    "먼저 비행거리가 1,000마일 이상인 모든 항공편?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14bf6b14-38c9-4b13-8d9d-b6c27f659299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT distance, origin, destination\n",
    "FROM us_delay_flights_tbl WHERE distance > 1000\n",
    "ORDER BY distance DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ddcb86-b24b-4e05-a249-5950a8ef2e78",
   "metadata": {},
   "source": [
    "가장 긴 비행은 호놀룰루(NHL)와 뉴욕(JFK)이었다. <br>\n",
    "다음으로 샌프란시스코(SFO)와 시카고(ORD) 간 2시간 이상 지연이 있었던 모든 항공편을 찾아보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26df4e59-bfe1-41a6-b979-f631d05c2ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT date, delay, origin, destination\n",
    "FROM us_delay_flights_tbl \n",
    "WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD'\n",
    "ORDER BY delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d7d451-d745-49ea-a8e0-fc5eae925ab2",
   "metadata": {},
   "source": [
    "이 두 도시간에 서로 다른 날짜에도 상당히 많은 지연 항공편이 있었던 것으로 보인다. <br>\n",
    "연습으로 date 칼럼을 읽을 수 있는 포맷으로 변경하고, 가장 흔하게 지연이 발생한 날짜나 달을 찾아보라. 항공 지연이 겨울 또는 휴일과 관련된 것인가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bce1c8b3-9c2c-4859-8449-aa0b9b4a506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# df.withColumn('date', to_timestamp(col('date'), format = 'MddHHmm')).show()\n",
    "\n",
    "# 오류 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c25e79f4-7603-4dd8-b7f3-e96c51c82ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adj = df.withColumn('date', concat(expr('0'), expr('date')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2ef7137c-0877-4e7c-9ccd-8075a3be8964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adj_tmst = df_adj.withColumn('date', to_timestamp(col('date'), format = 'MMddHHmm'))\n",
    "# format에 '월'을 MM으로 읽어야 했다.\n",
    "# 따라서 0을 'date'컬럼에 맨 앞에 추가했음\n",
    "# 책에 나온 데이터에는 0이 있었는데 다운받은 데이터에는 없네.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d37d0daa-2a91-4af7-af47-a57fc3f1d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adj_mth = df_adj_tmst.withColumn('monthdate', month(col('date')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "103176ed-bc0c-41e3-9af0-90072e75fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adj_mth.createOrReplaceTempView('us_delay_flights_adj_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7e9ae5a3-53db-4351-9ca2-8fade3d71308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|               date|COUNT|\n",
      "+-------------------+-----+\n",
      "|1970-01-06 06:00:00|  109|\n",
      "|1970-01-04 06:00:00|  109|\n",
      "|1970-01-06 07:00:00|  105|\n",
      "|1970-01-04 07:00:00|  101|\n",
      "|1970-01-02 07:00:00|   94|\n",
      "|1970-01-08 07:00:00|   93|\n",
      "|1970-01-05 06:00:00|   93|\n",
      "|1970-01-02 06:00:00|   93|\n",
      "|1970-01-07 07:00:00|   89|\n",
      "|1970-03-04 06:00:00|   84|\n",
      "+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT date, COUNT(*) AS COUNT\n",
    "FROM us_delay_flights_adj_tbl\n",
    "WHERE delay > 0\n",
    "GROUP BY date\n",
    "ORDER BY COUNT DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4970fdf4-944c-4f9d-aad1-9457930e1e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|monthdate| COUNT|\n",
      "+---------+------+\n",
      "|        1|208606|\n",
      "|        3|201046|\n",
      "|        2|182075|\n",
      "+---------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT monthdate, COUNT(*) AS COUNT\n",
    "FROM us_delay_flights_adj_tbl\n",
    "WHERE delay > 0\n",
    "GROUP BY monthdate\n",
    "ORDER BY COUNT DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a02f1d-f036-4f74-b0f8-6f40ed12a3e7",
   "metadata": {},
   "source": [
    "SQL의 CASE 절을 사용하는 좀 더 복잡한 쿼리를 시도해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fb0b4874-9748-41bf-a83c-bc21a6a0a7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    "            CASE\n",
    "                WHEN delay > 360 THEN 'Very Long Delays'\n",
    "                WHEN delay >= 120 AND delay <= 360 THEN 'Long Delays'\n",
    "                WHEN delay >= 60 AND delay <= 120 THEN 'Short Delays'\n",
    "                WHEN delay >= 0 AND delay <= 60 THEN 'Tolerable Delays'\n",
    "                WHEN delay = 0 THEN 'No Delays'\n",
    "                ELSE 'Early'\n",
    "            END AS Flight_Delays\n",
    "            FROM us_delay_flights_tbl\n",
    "            ORDER BY origin, delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcccc17-5037-4081-9b7b-c1fa34ee012b",
   "metadata": {},
   "source": [
    "spark.sql 인터페이스를 사용하면 3장에서 알아본 것과 같은 일반적인 데이터 분석 작업을 수행할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee1addf-177f-4fc4-b4ec-b4620be51f2e",
   "metadata": {},
   "source": [
    "# SQL 테이블과 뷰\n",
    "스파크는 각 테이블과 해당 데이터에 관련된 정보인 스키마, 설명, 테이블명, 데이터베이스명, 칼럼명, 파티션, 실제 데이터의 물리적 위치 등의 메타데이터를 가지고 있다.(다른 데이터를 정의하고 기술하는 데이터)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0796277f-0138-4f80-ab47-d6a8133dc1eb",
   "metadata": {},
   "source": [
    "## 관리형 테이블과 비관리형 테이블\n",
    "스파크는 관리형(managed)과 비관리형(unmanaged)이라는 두 가지 유형의 테이블을 만들 수 있다.\n",
    "\n",
    "- 관리형 테이블의 경우 스파크는 메타데이터와 파일 저장소의 데이터를 모두 관리한다. 때문에 DROP TABLE <테이블명>과 같은 SQL명령은 메타데이터와 실제 데이터를 모두 삭제한다.\n",
    "- 비관리형 테이블의 경우에는 오직 메타데이터만 관리하고 카산드라와 같은 외부 데이터 소스에서 테이블을 직접 관리한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6806dec3-7bdb-4d86-bde4-8c898adcd1c8",
   "metadata": {},
   "source": [
    "## SQL 데이터베이스와 테이블 생성하기\n",
    "테이블은 데이터베이스 안에 존재한다. 기본적으로 스파크는 default 데이터베이스 안에 테이블을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03a48082-54b9-444e-b99c-57f1ed468494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터베이스 생성\n",
    "spark.sql('CREATE DATABASE learn_spark_db')\n",
    "# 스파크에게 해당 데이터베이스를 사용하겠다고 알려주기\n",
    "spark.sql('USE learn_spark_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9833f5-6af7-41f9-85e0-a3a999d446de",
   "metadata": {},
   "source": [
    "이 시점부터는 애플리케이션에 실행되는 어떠한 명령어든 learn_spark_db 데이터베이스 안에서 생성되고 상주하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d77a9c3-1f52-4c2c-b6b3-e637ce7bbd10",
   "metadata": {},
   "source": [
    "### 관리형 테이블 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e99f25a-5720-40e9-a2c7-31a30d461daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql('CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT, distance INT, \\\n",
    "# origin STRING, destination STRING)')\n",
    "\n",
    "# 에러발생\n",
    "# Hive support is required to CREATE Hive TABLE (AS SELECT).;\n",
    "# 'CreateTable `spark_catalog`.`learn_spark_db`.`managed_us_delay_flights_tbl`, \n",
    "# org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b701a4-e80f-491c-b823-428639235c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 14:27:57 WARN Utils: Your hostname, minseok-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/09/07 14:27:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/07 14:27:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "  .builder\n",
    "  .appName(\"Hive External Table\")\n",
    "  .enableHiveSupport() # 에러 해결\n",
    "  .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "778e3274-3af2-4757-b60b-46ec285af9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 14:28:57 WARN ObjectStore: Failed to get database learn_spark_hive_solve_db, returning NoSuchObjectException\n",
      "23/09/07 14:28:57 WARN ObjectStore: Failed to get database learn_spark_hive_solve_db, returning NoSuchObjectException\n",
      "23/09/07 14:28:57 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "23/09/07 14:28:57 WARN ObjectStore: Failed to get database learn_spark_hive_solve_db, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터베이스 새로 생성\n",
    "spark.sql('CREATE DATABASE learn_spark_hive_solve_db')\n",
    "# 스파크에게 해당 데이터베이스를 사용하겠다고 알려주기\n",
    "spark.sql('USE learn_spark_hive_solve_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce10175-daad-4c2d-8b92-d01928f7094d",
   "metadata": {},
   "source": [
    "CREATE TABLE로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c888fd4-1696-4f96-9056-690bf0ae48d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 14:29:11 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/09/07 14:29:11 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/09/07 14:29:12 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/09/07 14:29:12 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/09/07 14:29:12 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/09/07 14:29:12 WARN HiveMetaStore: Location: file:/home/minseok/spark-3.4.1-bin-hadoop3/python/Learning_Spark/Chapter4/spark-warehouse/learn_spark_hive_solve_db.db/managed_us_delay_flights_tbl specified for non-external table:managed_us_delay_flights_tbl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT, distance INT, \\\n",
    "origin STRING, destination STRING)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b4c8d8-9de8-49b5-b62b-42522d6f3cdf",
   "metadata": {},
   "source": [
    "또한 데이터 프레임 API를 아래와 같이 사용하여 같은 명령을 수행할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e2d24b7-0b49-4860-9355-ca20108137cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 미국 항공편 지연 csv 파일 경로\n",
    "csv_file = 'departuredelays.csv'\n",
    "# 앞에서 정의한 스키마\n",
    "schema = \"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\n",
    "flights_df = spark.read.csv(csv_file, schema = schema)\n",
    "flights_df.write.saveAsTable('managed_us_delay_flights_tbl_2')\n",
    "\n",
    "# 위 cell과 테이블이름이 같으면 에러난다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305cc06b-b240-4fde-a8bf-b930f683e5f3",
   "metadata": {},
   "source": [
    "### 비관리형 테이블 생성하기\n",
    "스파크 애플리케이션에서 접근 가능한 파일 저장소에 있는 파케이, csv, 및 JSON 파일 포맷의 데이터 소스로부터 비관리형 테이블을 생성할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70569e0f-c71a-4324-84ed-a7f0027b77da",
   "metadata": {},
   "source": [
    "CREATE TABLE로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73a3d260-1439-49a5-99ad-18a1ab9aabdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 14:45:30 WARN HadoopFSUtils: The directory file:/home/minseok/spark-3.4.1-bin-hadoop3/python/Learning_Spark/Chapter4/depardeparturedelays.csv was not found. Was it deleted very recently?\n",
      "23/09/07 14:45:30 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider csv. Persisting data source table `spark_catalog`.`learn_spark_hive_solve_db`.`unmanaged_us_delay_flights_tbl` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE unmanaged_us_delay_flights_tbl (\n",
    "    date STRING,\n",
    "    delay INT,\n",
    "    distance INT,\n",
    "    origin STRING, \n",
    "    destination STRING\n",
    ")\n",
    "USING csv OPTIONS (PATH 'depardeparturedelays.csv')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf64dd4-901c-4695-97a6-cebf04c011b1",
   "metadata": {},
   "source": [
    "데이터 프레임 API에서는 다음과 같은 명령어를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed333769-e90b-495f-9a9e-029499572df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(flights_df\n",
    " .write\n",
    " .option('path', '/tmp/data/us_flights_delay')\n",
    " .saveAsTable(\"unmanaged_us_delay_flights_tbl_2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cce2b8e-ddcf-4d73-8477-c19caab63f73",
   "metadata": {},
   "source": [
    "## 뷰 생성하기\n",
    "뷰를 생성한 후에는 테이블처럼 쿼리할 수 있다. 뷰는 테이블과 달리 실제로 데이터를 소유하지 않기 때문에 스파크 애플리케이션이 종료되면 테이블은 유지되지만 뷰는 사라진다. <br>\n",
    "미국 비행 지연 데이터에서 뉴욕(JFK) 및 샌프란시스코(SFO)가 출발지인 공항이 있는 하위 데이터세트에 대해서만 작업하려는 경우 다음 쿼리는 해당 테이블의 일부로 전역 임시 뷰 및 일반 임시 뷰를 생성한다. <br>\n",
    "임시 뷰는 스파크 애플리케이션 내의 단일 SparkSession에 연결된다. 반면에 전역 임시 뷰는 스파크 애플리케이션 내의 여러 SparkSession에서 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849f1032-f38e-4362-a3ad-5409759baf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''SQL 쿼리\n",
    "CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view AS\n",
    "    SELECT date, delay, origin, destination FROM us_delay_flights_tbl\n",
    "    WHERE origin = 'SFO';\n",
    "\n",
    "CREATE OR REPLACE TEMP VIEW us_origin_airport_JFK_tmp_view AS\n",
    "    SELECT date, delay, origin, destination FROM us_delay_flights_tbl\n",
    "    WHERE origin = 'JFK';\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8de2c3-6fcd-4c31-b676-97acd4a93380",
   "metadata": {},
   "source": [
    "다음과 같은 데이터 프레임 API를 통해 같은 결과를 도출할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96b0eb96-9d8f-4b46-9e0b-8606ee68545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sfo = spark.sql(\"SELECT date, delay, origin, destination \\\n",
    "FROM us_delay_flights_tbl WHERE origin = 'SFO'\")\n",
    "    \n",
    "df_jfk = spark.sql(\"SELECT date, delay, origin, destination \\\n",
    "FROM us_delay_flights_tbl WHERE origin = 'JFK'\")\n",
    "\n",
    "# 임시 뷰와 전역 임시 뷰 생성\n",
    "df_sfo.createOrReplaceGlobalTempView('us_origin_airport_SFO_global_tmp_view')\n",
    "df_jfk.createOrReplaceTempView('us_origin_airport_JFK_tmp_view')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d00755-b2c2-43ac-97eb-47de789d5bde",
   "metadata": {},
   "source": [
    "스파크는 global_temp라는 전역 임시 데이터베이스에 전역 임시 뷰를 생성하므로 해당 뷰에 액세스할 때는 global_temp.`<view_name>` 접두사를 사용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a008cb2c-0467-4e35-8ef6-0fa100b87a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''SQL 쿼리\n",
    "SELECT * FROM global_temp.us_origin_airport_SFO_global_tmp_view\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79e68b2-e177-449f-8678-3c0026057f10",
   "metadata": {},
   "source": [
    "반면에 일반 임시 뷰는 global_temp 접두사 없이 접근할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b77ace3-a530-43b3-8345-a661d1ca800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''SQL 쿼리\n",
    "SELECT * FROM us_origin_airport_JFK_tmp_view\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7f9d339-1f9e-43d3-a198-b0d83a3fa2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: int, delay: int, origin: string, destination: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.table('us_origin_airport_JFK_tmp_view')\n",
    "# 또는\n",
    "# spark.sql('SELECT * us_origin_airport_JFK_tmp_view')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ea01e7-0881-4d6d-b051-ac98f32fa51e",
   "metadata": {},
   "source": [
    "또한 테이블처럼 뷰도 드롭할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8418c7-e2d9-462f-b470-3e583129e70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''SQL 쿼리\n",
    "DROP VIEW IF EXISTS us_origin_airport_SFO_global_tmp_view;\n",
    "DROP VIEW IF EXISTS us_origin_airport_JFK_tmp_view;\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8b0c732-e0d9-4c60-9355-ae28fa1fb989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.dropGlobalTempView('us_origin_airport_SFO_global_tmp_view')\n",
    "spark.catalog.dropTempView('us_origin_airport_JFK_tmp_view')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60cace1-f0e3-4008-9a65-ffb62bed5a9c",
   "metadata": {},
   "source": [
    "## 메타데이터 보기\n",
    "메타데이터는 스파크 SQL의 상위 추상화 모듈인 카탈로그에 저장된다.<br>\n",
    "다음과 같은 함수를 통해 저장된 모든 메타데이터에 엑세스할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59440b2c-4b06-43be-98d7-116168a74bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='file:/home/minseok/spark-3.4.1-bin-hadoop3/python/Learning_Spark/Chapter4/spark-warehouse'),\n",
       " Database(name='learn_spark_db', catalog='spark_catalog', description='', locationUri='file:/home/minseok/spark-3.4.1-bin-hadoop3/python/Learning_Spark/Chapter4/spark-warehouse/learn_spark_db.db'),\n",
       " Database(name='learn_spark_hive_solve_db', catalog='spark_catalog', description='', locationUri='file:/home/minseok/spark-3.4.1-bin-hadoop3/python/Learning_Spark/Chapter4/spark-warehouse/learn_spark_hive_solve_db.db')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41e7d0d5-cd97-4dff-a412-cb170094eccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='managed_us_delay_flights_tbl', catalog='spark_catalog', namespace=['learn_spark_hive_solve_db'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='managed_us_delay_flights_tbl_2', catalog='spark_catalog', namespace=['learn_spark_hive_solve_db'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='unmanaged_us_delay_flights_tbl', catalog='spark_catalog', namespace=['learn_spark_hive_solve_db'], description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='unmanaged_us_delay_flights_tbl_2', catalog='spark_catalog', namespace=['learn_spark_hive_solve_db'], description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='us_delay_flights_tbl', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='us_origin_airport_JFK_global_tmp_view', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ccca007-3c0c-4381-ae45-0ac225506d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='date', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='delay', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='distance', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='origin', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='destination', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns('us_delay_flights_tbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c2f404-84a0-4a00-af4e-a63de5a1df8c",
   "metadata": {},
   "source": [
    "### 테이블을 데이터 프레임으로 읽기\n",
    "SQL을 사용하여 테이블을 쿼리하고 반환된 결과를 데이터 프레임에 저장할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9182cc8-7082-4a98-b755-eef827a53a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: int, delay: int, distance: int, origin: string, destination: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_flights_df = spark.sql('SELECT * FROM us_delay_flights_tbl')\n",
    "us_flights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "240d47d7-527b-4d0b-afec-6bfcf43287e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: int, delay: int, distance: int, origin: string, destination: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_flights_df2 = spark.table('us_delay_flights_tbl')\n",
    "us_flights_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4393ab29-ea41-4ebd-9374-33fabc737d3d",
   "metadata": {},
   "source": [
    "# 데이터 프레임 및 SQL 테이블을 위한 데이터 소스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d04c1-13bf-45c8-b271-9efddf6b30a8",
   "metadata": {},
   "source": [
    "## DataFrameReader\n",
    "데이터 소스에서 데이터 프레임으로 데이터를 읽기 위한 핵심 구조 <br>\n",
    "정의된 형식과 권장되는 사용 패턴이 있다.\n",
    "> DataFrameReader.format(args).option('key', 'value').schema(args).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d15a46-2b57-4074-9ac3-7f4ce04368ca",
   "metadata": {},
   "source": [
    "오직 SparkSession 인스턴스를 통해서만 DataFrameReader에 엑세스할 수 있다. <br>\n",
    "인스턴스 핸들을 얻기 위해서는 다음을 사용해라\n",
    ">SparkSession.read (정적 데이터 소스에서 DataFrame으로 읽기 위해 DataFrameReader에 대한 핸들을 반환)<br>\n",
    ">또는 <br>\n",
    ">SparkSession.readStream (스트리밍 소스에서 읽을 인스턴스를 반환 -> 정형화 스트리밍 파트)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc058a0f-3be8-4504-bd23-45e1728cf92b",
   "metadata": {},
   "source": [
    "DataFrameReader 함수,인수 및 옵션\n",
    "|함수|인수|설명|\n",
    "|-|-|-|\n",
    "|format()|'parquet','csv','txt','json','jdbc','orc','avro' 등|이 함수를 지정하지 않으면 기본값은 파케이 또는 spark.sql.sources.default에 지정된 항목으로 설정된다.|\n",
    "|option()|('mode','PERMISSIVE / FAILFAST / DROPMALFORMED') ('inferSchema', 'true / false') ('path','path_file_data_source')|일련의 키/값 쌍 및 옵션이다. 기본 모드는 PERMISSIVE이다. 'inferSchema' 및 'mode' 옵션은 JSON 및 CSV파일 형식에만 적용된다.|\n",
    "|schema()|DDL 문자열 또는 StructType|JSON 및 CSV 형식의 경우 option() 함수에서 스키마를 유추하도록 지정할 수 있다.('inferSchema')|\n",
    "|load()|'/path/to/data/source'|데이터 소스의 경로이다. option('path','...')에 지정된 경우 비워둘 수 있다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff08c007-a1b4-4da1-a522-e57d75958057",
   "metadata": {},
   "source": [
    "## DataFrmaeWriter\n",
    "DataFrameWriter는 DataFrameReader와 달리 SparkSession이 아닌 저장하려는 데이터 프레임에서 인스턴스에 액세스가 가능하다. 권장되는 사용 형식은 다음과 같다\n",
    "> DataFrameWriter.format(args) <br>\n",
    "> .option(args) <br>\n",
    "> .bucketBy(args) <br>\n",
    "> .partitionBy(args) <br>\n",
    "> .save(path)\n",
    ">\n",
    "> DataFrameWriter.format(args).option(args).sortBy(args).saveAsTable(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70cc554-bfc4-466b-94e8-7e6f37d27b8a",
   "metadata": {},
   "source": [
    "인스턴스 핸들을 얻기 위해서는 다음을 사용해라\n",
    "> DataFrame.write\n",
    "> 또는\n",
    "> DataFrame.writeStream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03a8321-2f28-4ef1-9099-3f63427c248e",
   "metadata": {},
   "source": [
    "DataFrameWriter 함수,인수 및 옵션\n",
    "|함수|인수|설명|\n",
    "|-|-|-|\n",
    "|format()|'parquet','csv','txt','json','jdbc','orc','avro' 등|이 함수를 지정하지 않으면 기본값은 파케이 또는 spark.sql.sources.default에 지정된 항목으로 설정된다.|\n",
    "|option()|('mode','append / overwrite / ignore / error or errorifexists') ('mode', 'SaveMode.Overwrite / SaveMode.Append, SaveMode.Ignore, SaveMode.ErrorIfExists') ('path','path_to_write_to')|일련의 키/값 쌍 및 옵션이다. 기본 모드 옵션은 error 또는 errorifexists와 SaveMode이다. ErrorIfExists는 데이터가 이미 있는 경우 런타임에서 예외를 발생시킨다.|\n",
    "|bucketBy()|(numBuckets, col, col..., coln)|버킷 개수 및 버킷 기준 칼럼 이름이다. 파일 시스템에서 하이브의 버킷팅 체계를 사용한다.|\n",
    "|save()|'/path/to/data/source'|데이터 소스의 경로이다. option('path','...')에 지정된 경우 비워둘 수 있다.|\n",
    "|saveAsTable()|'table_name'|저장할 테이블이다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5358c3fd-f7f2-48cd-a3ee-af64fb57c33a",
   "metadata": {},
   "source": [
    "## 파케이\n",
    "일반적으로 정적 파케이 데이터 소스에서 읽을 때는 스키마가 필요하지 않다. 파케이 메타데이터는 보통 스키마를 포함하므로 스파크에서 스키마를 파악할 수 있다. 그러나 스트리밍 데이터 소스의 경우에는 스키마를 제공해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcd9c1d-d43c-4a65-9e4c-3e06e579aec3",
   "metadata": {},
   "source": [
    "### 파케이 파일을 데이터 프레임으로 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1df66ca-355f-4477-8f62-e367b042de95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[CalNumber: int, UnitID: string, IncidentNumber: int, CallType: string, CallDate: string, WatchDate: string, CallFinalDisposition: string, AvailableDtTm: string, Address: string, City: string, Zipcode: int, Battalion: string, StationArea: string, Box: string, OriginalPriority: string, Priority: string, FinalPriority: int, ALSUnit: boolean, CallTypeGroup: string, NumAlarms: int, UnitType: string, UnitSequenceCallDispatch: int, FirePreventionDistrict: string, SupervisorDistrict: string, Neighborhood: string, Location: string, RowID: string, Delay: float]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"\"\"../Chapter3/파케이저장연습/\"\"\"\n",
    "df = spark.read.format('parquet').load(file)\n",
    "# 파일이 위치한 폴더를 경로로 지정해야 하네\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba09589-11f1-4dbe-a8d5-7c066ccb241a",
   "metadata": {},
   "source": [
    "### 파케이 파일을 Spark SQL 테이블로 읽기\n",
    "파케이 파일을 데이터 프레임으로 읽을 뿐만 아니라 스파크 SQL 비관리형 테이블 또는 뷰를 직접 만들 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2b41344-eefe-47f6-a5a5-4fff580b4ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"SQL 예제\n",
    "CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "    USING parquet\n",
    "    OPTIONS (\n",
    "        path \"../Chapter3/파케이저장연습/\" ) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30fe975d-490c-40f5-a7fc-ddc8681ea1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1020600|   -8|     369|   ABE|        DTW|\n",
      "|1021245|   -2|     602|   ABE|        ATL|\n",
      "|1020605|   -4|     602|   ABE|        ATL|\n",
      "|1031245|   -4|     602|   ABE|        ATL|\n",
      "|1030605|    0|     602|   ABE|        ATL|\n",
      "|1041243|   10|     602|   ABE|        ATL|\n",
      "|1040605|   28|     602|   ABE|        ATL|\n",
      "|1051245|   88|     602|   ABE|        ATL|\n",
      "|1050605|    9|     602|   ABE|        ATL|\n",
      "|1061215|   -6|     602|   ABE|        ATL|\n",
      "|1061725|   69|     602|   ABE|        ATL|\n",
      "|1061230|    0|     369|   ABE|        DTW|\n",
      "|1060625|   -3|     602|   ABE|        ATL|\n",
      "|1070600|    0|     369|   ABE|        DTW|\n",
      "|1071725|    0|     602|   ABE|        ATL|\n",
      "|1071230|    0|     369|   ABE|        DTW|\n",
      "|1070625|    0|     602|   ABE|        ATL|\n",
      "|1071219|    0|     569|   ABE|        ORD|\n",
      "|1080600|    0|     369|   ABE|        DTW|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM us_delay_flights_tbl').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0206d9d8-ceae-412e-a142-a44467d0078b",
   "metadata": {},
   "source": [
    "### 데이터 프레임을 파케이 파일로 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbe1eff6-3364-499b-8b96-444fa3580b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 16:43:20 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(df.write.format('parquet')\n",
    " .mode('overwrite')\n",
    " .option('compression', 'snappy')\n",
    " .save('/tmp/data/parquet/df_parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d6ad8-97ec-43e7-8aaf-2d0ff8acf721",
   "metadata": {},
   "source": [
    "이렇게 하면 지정된 경로에 압축된 파케이 파일 집합이 생성된다. 압축 방ㅅ익으로 snappy를 사용했으므로 snappy 압축 파일이 생성될 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1296e582-c9ee-467b-8a53-15be857949cb",
   "metadata": {},
   "source": [
    "### 스파크 SQL 테이블에 데이터 프레임 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2bc2d46f-0046-49fb-a911-6853d976a653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(df.write\n",
    " .mode('overwrite')\n",
    " .saveAsTable('us_delay_flights_tbl'))\n",
    "# 'us_delay_flights_tbl'이라는 관리형 테이블이 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3247fe-266d-4fd6-9be8-bc892c1eca95",
   "metadata": {},
   "source": [
    "### JSON, CSV, 에이브로, ORC, 이미지, 이진 파일\n",
    "파케이와 비슷하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae44dc09-042d-4575-b52b-84c74502156f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18969bfd-18b7-4692-a38e-8cb768d50e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8a2f4e-f46e-4841-9a74-901c23e494c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9742063-30f5-4cf7-af38-4315309054a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d0c897-cbd6-4a8d-b089-7837d2c96e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94812930-20de-4d62-983f-6609e7ce12eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f82084-e1c8-44a1-99f8-748ba0cebcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527409d7-6a8c-4ae2-97a2-27e4c789d28a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
