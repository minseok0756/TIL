{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25345ef2-b725-4873-8ff6-6117442ae6c6",
   "metadata": {},
   "source": [
    "# 스파크 SQL과 아파치 하이브"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5a485c-68c1-48e3-a7eb-49de156c88ea",
   "metadata": {},
   "source": [
    "## 사용자 정의 함수(UDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7258913-f726-465c-abb5-0e7f6ec93d07",
   "metadata": {},
   "source": [
    "### 스파크 SQL UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddc50143-4a29-49dd-bfb0-a48c5675e0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/08 10:13:00 WARN Utils: Your hostname, minseok-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/09/08 10:13:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/08 10:13:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('chapter5').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b2d8151-c713-410e-ad10-5f4acafdb6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 큐브 함수 생성\n",
    "def cubed(s):\n",
    "    return s * s * s\n",
    "\n",
    "# UDF로 등록\n",
    "spark.udf.register('cubed', cubed, LongType())\n",
    "\n",
    "# 임시 뷰 생성\n",
    "spark.range(1, 9).createOrReplaceTempView('udf_test')\n",
    "# spark.range(1, 9)\n",
    "# DataFrame[id: bigint]\n",
    "# spark.range(1, 9).show()\n",
    "# +---+\n",
    "# | id|\n",
    "# +---+\n",
    "# |  1|\n",
    "# |  2|\n",
    "# |  3|\n",
    "# |  4|\n",
    "# |  5|\n",
    "# |  6|\n",
    "# |  7|\n",
    "# |  8|\n",
    "# +---+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76197193-495e-4ed1-a65d-7a0fc69e1f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|id_cubed|\n",
      "+---+--------+\n",
      "|  1|       1|\n",
      "|  2|       8|\n",
      "|  3|      27|\n",
      "|  4|      64|\n",
      "|  5|     125|\n",
      "|  6|     216|\n",
      "|  7|     343|\n",
      "|  8|     512|\n",
      "+---+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 함수 사용\n",
    "spark.sql(\"SELECT id, cubed(id) AS id_cubed FROM udf_test\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62852fba-cc14-4b34-805a-763824c6fb7e",
   "metadata": {},
   "source": [
    "### 판다스 UDF로 파이스파크 UDF 속도 향상 및 배포\n",
    "파이스파크 UDF 사용은 성능이 느리다는 단점이 있다. <br>\n",
    "이 문제를 해결하기 위해 판다스 UDF가 도입되었다. 판다스 UDF는 아파치 애로우를 사용하여 데이터를 전송하고 판다스는 해당 데이터로 작업을 한다.pandas_udf 키워드를 데코레이터로 사용하여 판다스 UDF를 정의하거나 함수 자체를 래핑할 수 있다. 한 번에 한 행씩 처리하는 파이썬 UDF에 비해 최대 100배가지 성능을 향상 시킬 수 있는 벡터화된 연산을 허용한다.<br>\n",
    "판다스 UDF는 파이썬 3.6 이상 기반의 아파치 스파크 3.0에서 판다스 UDF 및 판다스 함수 API로 분할되었다.\n",
    "\n",
    "- 판다스 UDF: 아파치 스파크 3.0에서 판다스 UDF는 pandas.Series, pandas.DataFrame, Tuple 및 Iterator와 같은 파이썬 유형 힌트로 판다스 UDF 유형을 유추한다. 현재 판다스 UDF에서는 시리즈와 시리즈, 시리즈 반복자와 시리즈 반복자, 다중 시리즈 반복자와 시리즈 반복자, 시리즈와 스칼라(단일값)을 파이썬 유형 힌트로 지원한다.\n",
    "- 판다스 함수 API: 판다스 함수 API를 사용하면 파이스파크 데이터 프레임에 입력과 출력이 모두 판다스 인스턴스인 로컬 파이썬 함수를 직접 적용할 수 있다. 스파크 3.0의 경우 판다스 함수 API는 그룹화된 맵, 맵, 공동 그룹화된 맵을 지원한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0ee447a-2a3a-412f-bd94-06e66458dc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, pandas_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9e4ae90-7878-4b38-a4e6-90e5480f15ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40e21c5b-e226-4621-a940-e670ae308e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 큐브 함수 선언\n",
    "# def cubed(a: pd.Series) -> pd.Series:\n",
    "#     return a * a * a\n",
    "\n",
    "# # 큐브 함수에 대한 판다스 UDF 생성\n",
    "# cubed_udf = pandas_udf(cubed, returnType=LongType())\n",
    "\n",
    "# 오류 발생\n",
    "# PyArrow >= 1.0.0 must be installed; however, it was not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a08588ea-ae23-4bde-9783-ed0134353641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3752f021-7eef-4eef-a664-9a7a1b076795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 큐브 함수 선언\n",
    "# 판다스 UDF를 만들기 위한 일반적인 판다스 함수\n",
    "def cubed(a: pd.Series) -> pd.Series:\n",
    "    return a * a * a\n",
    "\n",
    "# 큐브 함수에 대한 판다스 UDF 생성\n",
    "cubed_udf = pandas_udf(cubed, returnType=LongType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289b6ea2-95f1-4e70-a089-60c6bd501d2c",
   "metadata": {},
   "source": [
    "큐브 계산을 위해 간단한 판다스 시리즈로 로컬 함수 cubed()를 적용해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9fc68d4-0de1-4e1e-8ce1-a7fa0e3587f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1\n",
      "1     8\n",
      "2    27\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 판다스 시리즈 생성\n",
    "x = pd.Series([1, 2, 3])\n",
    "\n",
    "# 로컬 판다스 데이터를 실행하는 pandas_udf에 대한 함수\n",
    "print(cubed(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeb6351-b1ba-4d32-9afe-43ae88ce0dd4",
   "metadata": {},
   "source": [
    "스파크 데이터 프레임으로 전환해보자. 이 함수를 다음과 같이 벡터화된 스파크 UDF로 실행할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58f642ea-7206-4c4f-9873-4ef1ef7303fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minseok/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:229: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "[Stage 2:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|cubed(id)|\n",
      "+---+---------+\n",
      "|  1|        1|\n",
      "|  2|        8|\n",
      "|  3|       27|\n",
      "+---+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minseok/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:229: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 스파크 데이터 프레임 생성, 'spark'는 기존의 sparkSession과 같다.\n",
    "df = spark.range(1, 4)\n",
    "\n",
    "# 벡터화된 스파크 UDF를 함수로 실행\n",
    "df.select('id', cubed_udf(col('id'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14940d4a-0bdf-4583-b3ac-893d8423b9a2",
   "metadata": {},
   "source": [
    "자세한 내용은 판다스 사용자 정의 함수 문저를 참고<br>\n",
    "https://docs.databricks.com/spark/latest/spark-sql/udf-python-pandas.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea270b-4f91-4325-972c-eff0d3232877",
   "metadata": {},
   "source": [
    "### 판다스 UDF, 판다스 함수 API에 대한 상세한 내용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555933f2-ac3d-460e-8b44-e993046d5640",
   "metadata": {},
   "source": [
    "Pandas UDF와 Pandas함수 API의 차이점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d60f7104-f379-4839-8a58-ea33dee32c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|          LOG2(id)|\n",
      "+------------------+\n",
      "|              null|\n",
      "|               0.0|\n",
      "|               1.0|\n",
      "| 1.584962500721156|\n",
      "|               2.0|\n",
      "| 2.321928094887362|\n",
      "| 2.584962500721156|\n",
      "| 2.807354922057604|\n",
      "|               3.0|\n",
      "|3.1699250014423126|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pandas UDF\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf, log2, col\n",
    "\n",
    "@pandas_udf('long')\n",
    "def pandas_plus_one(s:pd.Series) -> pd.Series:\n",
    "    return s + 1\n",
    "\n",
    "# pandas_plus_one(\"id\") is identically treated as _a SQL expression_ internally.\n",
    "# Namely, you can combine with other columns, functions and expressions.\n",
    "spark.range(10).select(\n",
    "    pandas_plus_one(col(\"id\") - 1) + log2(\"id\") + 1).show()\n",
    "# log2() 함수는 밑이 2인 로그함수임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec2ee648-6036-4a42-bab5-f11aee2d4c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minseok/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:229: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pandas Function API\n",
    "\n",
    "from typing import Iterator\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def pandas_plus_one(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
    "    return map(lambda v: v + 1, iterator)\n",
    "# 위 함수는 일반 파이썬 함수임\n",
    "\n",
    "\n",
    "# pandas_plus_one is just a regular Python function, and mapInPandas is\n",
    "# logically treated as _a separate SQL query plan_ instead of a SQL expression. \n",
    "# Therefore, direct interactions with other expressions are impossible.\n",
    "spark.range(10).mapInPandas(pandas_plus_one, schema=\"id long\").show()\n",
    "\n",
    "# 함수가 실행되는 원리는 아직 잘 모르겠음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b072b669-9390-4493-9e0b-7d8ec820e931",
   "metadata": {},
   "source": [
    "또한, 판다스 UDF는 파이썬 타입 힌트가 필요하지만, 판다스 함수 API의 타입 힌트는 현재 선택 사항이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e4cf5d-8512-408d-a11b-5aee85a9e6c0",
   "metadata": {},
   "source": [
    "현재 Pandas UDF에서 지원되는 파이썬 유형 힌트\n",
    "\n",
    "- Series to Series\n",
    "- Iterator of Series to Iterator of Series\n",
    "- Iterator of Multiple Series to Iterator of Series\n",
    "- Series to Scalar (a single value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a90bcfd-edb5-41ad-8692-98e5d3bbab8f",
   "metadata": {},
   "source": [
    "타입 힌트는 모든 경우에 pandas.Series를 사용해야 한다. 그러나 입력 또는 출력 유형 힌트에 pandas.DataFrame을 대신 사용해야 하는 한 가지 변형이 있다. 입력 또는 출력 열이 StructType인 경우이다. 아래 예제를 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "785a5a21-0931-44f2-a44e-b1e7e63b3ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[long_col: bigint, string_col: string, struct_col: struct<col1:string>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [[1, \"a string\", (\"a nested string\",)]],\n",
    "    \"long_col long, string_col string, struct_col struct<col1 string>\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf647a1d-4834-49cb-b4e3-d9c510a0ec72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+\n",
      "|long_col|string_col|       struct_col|\n",
      "+--------+----------+-----------------+\n",
      "|       1|  a string|{a nested string}|\n",
      "+--------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c191617-2004-4609-9953-7e77693018f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+\n",
      "|pandas_plus_len(long_col, string_col, struct_col)|\n",
      "+-------------------------------------------------+\n",
      "|                             {a nested string, 9}|\n",
      "+-------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minseok/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:229: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"col1 string, col2 long\")\n",
    "def pandas_plus_len(\n",
    "        s1: pd.Series, s2: pd.Series, pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Regular columns are series and the struct column is a DataFrame.\n",
    "    # 타입 힌트 사용시 스파크 데이터프레임의 일반 칼럼은 Series로 사용하고 structtype인 칼럼은 DataFrame으로 사용된다.\n",
    "    # pdf: pd.DataFrame이므로 pdf는 structtype인 struct_col 칼럼을 의미한다.\n",
    "    pdf['col2'] = s1 + s2.str.len() \n",
    "    return pdf  # the struct column expects a DataFrame to return\n",
    "\n",
    "df.select(pandas_plus_len(\"long_col\", \"string_col\", \"struct_col\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31700ccf-a2f3-4ab0-a271-de5ac92ee759",
   "metadata": {},
   "source": [
    "지원되는 네 가지 유형 힌트에 대해 살펴본다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c399c10f-8c8b-45b4-953a-a821512616bf",
   "metadata": {},
   "source": [
    "- Series to Series <br>\n",
    "\n",
    "함수가 하나 이상의 pandas.Series를 취할 것으로 예상하고 하나의 pandas.Series를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05262e20-1257-4f42-89f9-231a90e0f7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minseok/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:229: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|pandas_plus_one(id)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  2|\n",
      "|                  3|\n",
      "|                  4|\n",
      "|                  5|\n",
      "|                  6|\n",
      "|                  7|\n",
      "|                  8|\n",
      "|                  9|\n",
      "|                 10|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minseok/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:229: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf       \n",
    "\n",
    "@pandas_udf('long')\n",
    "# @pandas_udf('long')은\n",
    "# cubed_udf = pandas_udf(cubed, returnType=LongType()) 과정을 축약한 과정으로 예상된다.\n",
    "def pandas_plus_one(s: pd.Series) -> pd.Series:\n",
    "    return s + 1\n",
    "\n",
    "spark.range(10).select(pandas_plus_one(\"id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee0af4f-9970-4f41-b71c-b33e0fa10aa9",
   "metadata": {},
   "source": [
    "- Iterator of Series to Iterator of Series <br>\n",
    "\n",
    "pandas.Series의 이터레이터를 받아 출력한다. 전체 출력의 길이는 전체 입력의 길이와 같아야 한다. 전체 입력과 출력의 길이가 같으면 입력 이터레이터에서 데이터를 미리 가져올 수 있다. 함수는 단일 열을 입력으로 받는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c32b95ee-dd7d-473e-8d65-e364722e0c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|pandas_plus_one(id)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  2|\n",
      "|                  3|\n",
      "|                  4|\n",
      "|                  5|\n",
      "|                  6|\n",
      "|                  7|\n",
      "|                  8|\n",
      "|                  9|\n",
      "|                 10|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minseok/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:229: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/home/minseok/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:229: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf       \n",
    "\n",
    "@pandas_udf('long')\n",
    "def pandas_plus_one(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    return map(lambda s: s + 1, iterator)\n",
    "    # map은 파이썬 시간에 배운 함수\n",
    "    # iterator 각 요소를 함수로 처리한 후 반환\n",
    "\n",
    "spark.range(10).select(pandas_plus_one(\"id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9fbabf-03be-4261-a3a0-bfcff241969f",
   "metadata": {},
   "source": [
    "- Iterator of Multiple Series to Iterator of Series <br>\n",
    "\n",
    "위 경우와 유사한 특성과 제한을 갖는다. <br>\n",
    "It is also useful when to use some states and when to prefetch the input data. (이해할 수 없는 말) <br>\n",
    "전체 출력의 길이가 입력의 길이와 같아야 한다. 위 경우와 달리 여러 열을 입력으로 받는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b38fd534-8c51-4d55-b929-1aa361586ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|multiply_two(id, id)|\n",
      "+--------------------+\n",
      "|                   0|\n",
      "|                   1|\n",
      "|                   4|\n",
      "|                   9|\n",
      "|                  16|\n",
      "|                  25|\n",
      "|                  36|\n",
      "|                  49|\n",
      "|                  64|\n",
      "|                  81|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minseok/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:229: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/home/minseok/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:229: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from typing import Iterator, Tuple\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf       \n",
    "\n",
    "@pandas_udf(\"long\")\n",
    "def multiply_two(\n",
    "        iterator: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
    "    # return (a * b for a, b in iterator)\n",
    "    # 이번엔 return 형식이 바뀌었네, iterator이기만 하면 되는건가?\n",
    "    return [a * b for a, b in iterator]\n",
    "    # 같은 결과가 나왔다\n",
    "\n",
    "spark.range(10).select(multiply_two(\"id\", \"id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e965f13-1e53-4b8b-a37b-8ea0a576034d",
   "metadata": {},
   "source": [
    "- Series to Scalar <br>\n",
    "\n",
    "반환된 Scalar는 파이썬의 int, float 또는 numpy의 numpy.int64, numpy.float64 중 하나일 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b23239a-e5e4-48d3-bb29-86e979a7f904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minseok/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:229: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|pandas_mean(v)|\n",
      "+--------------+\n",
      "|          21.0|\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|pandas_mean(v)|\n",
      "+---+--------------+\n",
      "|  1|           3.0|\n",
      "|  2|          18.0|\n",
      "+---+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------+\n",
      "|pandas_mean(v) OVER (PARTITION BY id ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)|\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "|                                                                                           3.0|\n",
      "|                                                                                           3.0|\n",
      "|                                                                                          18.0|\n",
      "|                                                                                          18.0|\n",
      "|                                                                                          18.0|\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql import Window\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def pandas_mean(v: pd.Series) -> float:\n",
    "    return v.sum()\n",
    "\n",
    "df.select(pandas_mean(df['v'])).show()\n",
    "df.groupby(\"id\").agg(pandas_mean(df['v'])).show()\n",
    "df.select(pandas_mean(df['v']).over(Window.partitionBy('id'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2af77f-a77a-466c-824c-724d00c2f1c7",
   "metadata": {},
   "source": [
    "스파크 3.0에서 지원되는 Pandas 함수 API\n",
    "- grouped map\n",
    "- map\n",
    "- co-grouped map <br>\n",
    "\n",
    "the grouped map Pandas UDF는 group map Pandas Function API로 분류된다. <br>\n",
    "Pandas 함수 API의 파이썬 타입 힌트는 선택 사항이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424f404c-4ee6-4667-a169-2333ad741d22",
   "metadata": {},
   "source": [
    "- Grouped Map <br>\n",
    "\n",
    "Pandas 함수 API에서 grouped map은 그룹화된 데이터 프레임(df..groupby(...))에서 applyInPandas이다. <br>\n",
    "이 함수는 각 그룹을 함수안에 각 pandas.DataFrame에 매핑한다.(It maps each group to each pandas.DataFrame in the function.) <br>\n",
    "출력이 입력과 동일한 길이일 필요는 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8248d35-0b25-4b37-867f-f6d61cf74a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|   v|\n",
      "+---+----+\n",
      "|  1|-0.5|\n",
      "|  1| 0.5|\n",
      "|  2|-3.0|\n",
      "|  2|-1.0|\n",
      "|  2| 4.0|\n",
      "+---+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minseok/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:229: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\n",
    "\n",
    "def subtract_mean(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 선택사항이지만 사용한 모습. 파이선 타입 힌트 사용을 권장\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v= v - v.mean())\n",
    "    # help(pd.DataFrame.assign)\n",
    "    # Returns a new object with all original columns in addition to new ones.\n",
    "    # Existing columns that are re-assigned will be overwritten.\n",
    "    # 이미 'v' 칼럼이 존재하므로 'v'의 칼럼값을 바꾸는 함수가 된다. 마치 withColumn처럼\n",
    "    # 바꾸는 방식은 v - v.mean()\n",
    "\n",
    "df.groupby(\"id\").applyInPandas(subtract_mean, schema=df.schema).show()\n",
    "# groupby와 groupBy는 같은 함수임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27b77ad-2802-402d-a0eb-6675e71ecefc",
   "metadata": {},
   "source": [
    "- Map <br>\n",
    "\n",
    "Map Pandas 함수 API는 데이터 프레임의 mapInPandas이다. 이 함수는 각 파티션의 모든 배치를 매핑하고 각각을 변환한다. 이 함수는 pandas.DataFrame의 이터레이터를 받아 pandas.DataFrame의 이터레이터를 출력한다. 출력 길이가 입력 크기와 일치할 필요는 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "14512996-6683-45d1-adb4-3fadbd627b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minseok/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:229: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  1| 21|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "import pandas as pd\n",
    "\n",
    "df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
    "\n",
    "def pandas_filter(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
    "    for pdf in iterator:\n",
    "        yield pdf[pdf.id == 1]\n",
    "\n",
    "df.mapInPandas(pandas_filter, schema=df.schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb8621f-1ea1-4fae-a3a9-1c745449323a",
   "metadata": {},
   "source": [
    "- Co-grouped Map <br>\n",
    "\n",
    "grouped map과 유사하게 각 그룹을 함수안에 각 pandas.DataFrame에 매핑하지만 공통 키로 다른 DataFrame과 그룹화한 다음 함수가 각 코그룹에 적용된다. 마찬가지로 출력 길이에는 제한이 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "451d06aa-c7de-4022-86d4-303e391624cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+---+\n",
      "|time| id| v1| v2|\n",
      "+----+---+---+---+\n",
      "|1201|  1|1.0|  x|\n",
      "|1202|  1|3.0|  x|\n",
      "|1201|  2|2.0|  y|\n",
      "|1202|  2|4.0|  y|\n",
      "+----+---+---+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minseok/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:229: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/home/minseok/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:229: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = spark.createDataFrame(\n",
    "    [(1201, 1, 1.0), (1201, 2, 2.0), (1202, 1, 3.0), (1202, 2, 4.0)],\n",
    "    (\"time\", \"id\", \"v1\"))\n",
    "df2 = spark.createDataFrame(\n",
    "    [(1201, 1, \"x\"), (1201, 2, \"y\")], (\"time\", \"id\", \"v2\"))\n",
    "\n",
    "def asof_join(left: pd.DataFrame, right: pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.merge_asof(left, right, on=\"time\", by=\"id\")\n",
    "\n",
    "df1.groupby(\"id\").cogroup(\n",
    "    df2.groupby(\"id\")\n",
    ").applyInPandas(asof_join, \"time int, id int, v1 double, v2 string\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643b8ed1-5999-4a52-8b4b-9ad45f04a418",
   "metadata": {},
   "source": [
    "# 외부 데이터 소스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e5820c-5680-4bf6-8af9-e64e2f4f038c",
   "metadata": {},
   "source": [
    "## JDBC 및 SQL 데이터베이스\n",
    "스파크 SQL에는 JDBC를 사용하여 다른 데이터베이스에서 데이터를 읽을 수 있는 데이터 소스 API가 포함되어 있다. <br>\n",
    "시작하려면 JDBC 데이터 소스에 대한 JDBC 드라이버를 지정해야 하며 스파크 클래스 경로에 있어야 한다. $SPARK_HOME 폴더에서 다음과 같은 명령을 실행한다. <br>\n",
    "`./bin/spark-shell --driver-class-path $database.jar --jars $database.jar` <br>\n",
    "데이터 소스 API를 사용하여 원격 데이터베이스의 테이블을 데이터 프레임 또는 스파크 SQL 임시 뷰로 로드할 수 있다. 사용자는 데이터 소스 옵션에서 JDBC 연결 속성을 지정할 수 있다. <br>\n",
    "\n",
    "[스파크가 지원하는 더 많은 일반적인 연결 속성](https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5235f301-b293-41e8-be2f-0f9f33c2b0cf",
   "metadata": {},
   "source": [
    "### 파티셔닝의 중요성\n",
    "모든 데이터가 하나의 드라이버 연결을 통해 처리되므로 성능을 크게 저하시킬 수 있을 뿐 아니라 소스 시스템의 리소스를 포화 상태로 만들 수 있다.<br>\n",
    "\n",
    "파티셔닝 연결 속성 <br>\n",
    "|속성명|설명|\n",
    "|-|-|\n",
    "|numPartitions|테이블 읽기 및 쓰기에서 병렬 처리를 위해 사용할 수 있는 최대 파티션 수이다.|\n",
    "|partitionColumn|파티션을 결정하는데 사용되는 칼럼이다. 숫자, 날짜 또는 타임스탬프 칼럼이어야 한다.|\n",
    "|lowerBound|파티션을 나눌 때 설정할 partitionColumn의 최솟값을 의미한다.|\n",
    "|upperBound|파티션을 나눌 때 설정할 partitionColumn의 최댓값을 의미한다.|\n",
    "\n",
    "> 예시\n",
    "> numPartitions: 10\n",
    "> lowerBound: 1000\n",
    "> upperBound: 10000\n",
    ">\n",
    "> 위와 같은 경우 파티션 크기는 1,000이 되고 10개의 파티션이 생성된다\n",
    "> 다음 10개의 쿼리를 실행하는 것과 동일하다.\n",
    ">\n",
    "> SELECT * FROM table WHERE partitionColumn BETWEEN 1000 AND 2000\n",
    "> SELECT * FROM table WHERE partitionColumn BETWEEN 2000 AND 3000\n",
    "> ...\n",
    "> SELECT * FROM table WHERE partitionColumn BETWEEN 9000 AND 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec1e19e-33ff-4d8f-af8d-2aa3c68a8bfe",
   "metadata": {},
   "source": [
    "## MySQL\n",
    "MySQL 데이터베이스에 연결하려면 메이븐 또는 [MySQL](https://mvnrepository.com/artifact/mysql/mysql-connector-java)에서 JBDC jar를 빌드하거나 다운로드한 후에(후자가 더 쉽다.) 클래스 경로에 추가한다. 그런 다음 해당 jar를 지정하여 스파크 셸 또는 pyspark을 시작한다.\n",
    "\n",
    "`./bin/pyspark --jars mysql-connector-java_8.0.16-bin.jar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bac2a5e-341a-4cfd-9a3b-1d6368d4ef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로드 함수를 사용하여 JDBC 소스로부터 데이터를 로드\n",
    "jdbcDF = (spark\n",
    "          .read\n",
    "          .format('jdbc')\n",
    "          .option('url', 'jdbc:mysql://[DBSERVER]:3306/[DATABASE]')\n",
    "          .option('driver', 'com.mysql.jdbcDriver')\n",
    "          .option('dbtable', '[TABLENAME]')\n",
    "          .option('user', '[USERNAME]')\n",
    "          .option('password', '[PASSWORD')\n",
    "          .load())\n",
    "\n",
    "# 저장 함수를 사용하여 JDBC 소스에 데이터를 저장\n",
    "(jdbcDF\n",
    "    .write\n",
    "    .format('jdbc')\n",
    "    .option('url', 'jdbc:mysql://[DBSERVER]:3306/[DATABASE]')\n",
    "    .option('driver', 'com.mysql.jdbcDriver')\n",
    "    .option('dbtable', '[TABLENAME]')\n",
    "    .option('user', '[USERNAME]')\n",
    "    .option('password', '[PASSWORD')\n",
    "    .save())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed62cf59-bded-47e6-bc66-4c596152e216",
   "metadata": {},
   "source": [
    "### PostgreSQL, 애저 코스모스 DB, MS SQL 서버도 비슷"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8b3db7-d676-4246-8856-0dc5cf74c0d7",
   "metadata": {},
   "source": [
    "### 기타 외부 데이터 소스\n",
    "- 아파치 카산드라\n",
    "- 스노우플레이크\n",
    "- 몽고DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91b9740-fe98-41d2-a7a8-297cfd56ca2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77fa36ad-660f-4e52-97c0-dd6469b0feca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22819b7-ef95-47b5-b0cc-5362fd178824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fff585-466d-4fbc-bc14-8ad4540cce07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cede1bfd-ca78-4160-9113-80f6a72ae88c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177ab790-d9aa-4f3e-827e-80df2684caf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0365ca8-7923-4f54-8b90-f0ce82159dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aada0e-1a09-4c6b-b817-e7e504d2b8af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dc3a75-72a4-4e5f-a84f-cd83c91fb494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6f4ebf-4c84-44b0-86a3-11b123ad1bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7b74d9-7c65-4239-b5e1-891ffc9f67ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f3a635-5516-4411-bc1d-180d4468775a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dba2be-c9cd-4451-8be4-567440f7f27d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d399f02-0627-4d08-8f88-3bdeb9ea2231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
