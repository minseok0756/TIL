{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53f3f76a",
   "metadata": {},
   "source": [
    "# 스키마와 데이터 프레임 만들기\n",
    "- 스키마는 데이터프레임을 위해 칼럼 이름과 연관된 데이터 타입을 정의한 것이다.\n",
    "\n",
    "<br>\n",
    "\n",
    "- 외부 데이터 소스에서 구조화된 데이터를 읽을 때 스키마를 가져오는 방식과 달리 스키마를 정의하는 것은 세가지 장점이 있다.\n",
    "> 스파크가 데이터 타입을 추축해야 하는 책임을 덜어 준다.<br>\n",
    "> 스파크가 스키마를 확정하기 위해 파일의 많은 부분을 읽어 들이려고 별도의 잡을 만드는 것을 방지한다. 데이터 파일이 큰 경우, 이는 비용과 시간이 많이 드는 작업이다.<br>\n",
    "> 데이터가 스키마와 맞지 않는 경우, 조기에 문제를 발견할 수 있다.\n",
    "\n",
    "<br>\n",
    "\n",
    "- 따라서 데이터 소스에서 큰 파일을 읽어야 한다면 가능한 한 반드시 스키마를 미리 지정해 두기를 권장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f05851",
   "metadata": {},
   "source": [
    "프로그래밍 스타일로 스키마 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eba97ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([StructField('author', StringType(), False),\n",
    "                    StructField('title', StringType(), False),\n",
    "                    StructField('pages', IntegerType(), False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d14417",
   "metadata": {},
   "source": [
    "DDL을 사용한 스키마 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4f52bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = 'author STRING, title STRING, pages INT'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b546023",
   "metadata": {},
   "source": [
    "스키마를 정의하여 데이터프레임 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ab41fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/06 16:32:45 WARN Utils: Your hostname, minseok-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/09/06 16:32:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/06 16:32:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# define schema for our data\n",
    "# 프로그래밍 스타일로 스키마 정의\n",
    "schema = StructType([\n",
    "   StructField(\"Id\", IntegerType(), False),\n",
    "   StructField(\"First\", StringType(), False),\n",
    "   StructField(\"Last\", StringType(), False),\n",
    "   StructField(\"Url\", StringType(), False),\n",
    "   StructField(\"Published\", StringType(), False),\n",
    "   StructField(\"Hits\", IntegerType(), False),\n",
    "   StructField(\"Campaigns\", ArrayType(StringType()), False)])\n",
    "\n",
    "# DDL로 스키마 정의\n",
    "# schema = \"'Id' INT, 'First' STRING, 'Last' STRING, 'Url' STRING, \\\n",
    "#     'Published' STRING, 'Hits' INT, 'Campaigns' ARRAY<STRING>\"\n",
    "\n",
    "#create our data\n",
    "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, \n",
    "         [\"twitter\", \"LinkedIn\"]],\n",
    "       [2, \"Brooke\",\"Wenig\",\"https://tinyurl.2\", \"5/5/2018\", 8908, \n",
    "        [\"twitter\", \"LinkedIn\"]],\n",
    "       [3, \"Denny\", \"Lee\", \"https://tinyurl.3\",\"6/7/2019\",7659, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "       [4, \"Tathagata\", \"Das\",\"https://tinyurl.4\", \"5/12/2018\", 10568, \n",
    "        [\"twitter\", \"FB\"]],\n",
    "       [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, \n",
    "        [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "       [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, \n",
    "        [\"twitter\", \"LinkedIn\"]]\n",
    "      ]\n",
    "# main program\n",
    "if __name__ == \"__main__\":\n",
    "   #create a SparkSession\n",
    "   spark = (SparkSession\n",
    "       .builder\n",
    "       .appName(\"Example-3_6\")\n",
    "       .getOrCreate())\n",
    "   # create a DataFrame using the schema defined above\n",
    "   blogs_df = spark.createDataFrame(data, schema)\n",
    "   # show the DataFrame; it should reflect our table above\n",
    "   blogs_df.show()\n",
    "   \n",
    "# 에러발생\n",
    "# 오류의 원인을 모르겠음.\n",
    "# 데이터프레임을 만들고 show하면 에러발생\n",
    "# read로 데이터를 읽어 show하면 에러가 발생하지 않음\n",
    "# ubuntu 환경에서 오류해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7623f3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = false)\n",
      " |-- First: string (nullable = false)\n",
      " |-- Last: string (nullable = false)\n",
      " |-- Url: string (nullable = false)\n",
      " |-- Published: string (nullable = false)\n",
      " |-- Hits: integer (nullable = false)\n",
      " |-- Campaigns: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "None\n",
      "StructType([StructField('Id', IntegerType(), False), StructField('First', StringType(), False), StructField('Last', StringType(), False), StructField('Url', StringType(), False), StructField('Published', StringType(), False), StructField('Hits', IntegerType(), False), StructField('Campaigns', ArrayType(StringType(), True), False)])\n"
     ]
    }
   ],
   "source": [
    "# print the schema used by Spark to process the DataFrame\n",
    "print(blogs_df.printSchema())\n",
    "\n",
    "# schema: 스키마 정의를 보여줌\n",
    "# 정의한 스키마를 복붙할 때 사용하기 좋음\n",
    "print(blogs_df.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31836c98",
   "metadata": {},
   "source": [
    "스키마를 미리 정의하고, 정의한대로 JSON파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12940afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# 프로그래밍 스타일로 스키마 정의\n",
    "# schema = StructType([\n",
    "#    StructField(\"Id\", IntegerType(), False),\n",
    "#    StructField(\"First\", StringType(), False),\n",
    "#    StructField(\"Last\", StringType(), False),\n",
    "#    StructField(\"Url\", StringType(), False),\n",
    "#    StructField(\"Published\", StringType(), False),\n",
    "#    StructField(\"Hits\", IntegerType(), False),\n",
    "#    StructField(\"Campaigns\", ArrayType(StringType()), False)])\n",
    "\n",
    "# DDL로 스키마 정의\n",
    "schema = \"Id INT, First STRING, Last STRING, Url STRING, \\\n",
    "    Published STRING, Hits INT, Campaigns ARRAY<STRING>\"\n",
    "\n",
    "# 지정한 스키마가 저장되어있는 변수를 넣어준다.\n",
    "blog_df = spark.read.schema(schema).json('blogs.json')\n",
    "blog_df.show()\n",
    "\n",
    "# 역시나 파일을 읽어보고 show하는것은 에러나지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84d729d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('Id', IntegerType(), True), StructField('First', StringType(), True), StructField('Last', StringType(), True), StructField('Url', StringType(), True), StructField('Published', StringType(), True), StructField('Hits', IntegerType(), True), StructField('Campaigns', ArrayType(StringType(), True), True)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(blog_df.printSchema())\n",
    "blog_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1accdd5",
   "metadata": {},
   "source": [
    "# 컬럼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91d68ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id', 'First', 'Last', 'Url', 'Published', 'Hits', 'Campaigns']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "blog_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f212d2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 값 계산을 위한 표현식 사용\n",
    "blog_df.select(expr('Hits * 2')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "306182ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# col을 이용한 계산\n",
    "blog_df.select(col('Hits') * 2).show(2)\n",
    "\n",
    "# 결과는 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "409f0787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 결과가 동일한 다른 표현\n",
    "blog_df.select(blog_df['Hits'] * 2).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a7fe4fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|Big Hitters|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|      false|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|      false|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|      false|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|       true|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|       true|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|       true|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 새로운 컬럼 생성\n",
    "blog_df.withColumn('Big Hitters', (expr('Hits > 10000'))).show()\n",
    "\n",
    "# (expr('Hits > 10000') 표현식에 맞는 값을 'Big Hitters'라는 새로운 컬럼으로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c542d013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    AuthorsId|\n",
      "+-------------+\n",
      "|  JulesDamji1|\n",
      "| BrookeWenig2|\n",
      "|    DennyLee3|\n",
      "|TathagataDas4|\n",
      "+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 다른 표현식\n",
    "(blog_df.withColumn('AuthorsId', (concat(expr('First'), expr('Last'), expr('Id'))))\n",
    " .select(col('AuthorsId'))\n",
    " .show(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5674c9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모두 같은 결과를 보여주는 표현\n",
    "blog_df.select(expr('Hits')).show(2)\n",
    "blog_df.select(col('Hits')).show(2)\n",
    "blog_df.select('Hits').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "75ba4f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 'Id' 컬럼값에 따라 역순으로 정렬\n",
    "blog_df.sort(col('Id').desc()).show()\n",
    "blog_df.orderBy(blog_df['Id'].desc()).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
