{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f28720",
   "metadata": {},
   "source": [
    "# 로우\n",
    "- Row는 스파크 객체이고 순서가 있는 필드 집합 객체이다.\n",
    "- 순서가 있으므로 Row의 각 필드를 0부터 시작하는 인덱스로 접근할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cb3c71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "blog_row = Row(6, 'Reynold', 'Xin', 'http', 255568, '3/2/2015', ['twit','LinkedIn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4efd84e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reynold'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인덱스로 개별 아이템에 접근한다\n",
    "blog_row[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33947699",
   "metadata": {},
   "source": [
    "Row 객체들은 빠른 탐색을 위해 데이터 프레임으로 만들어 사용하기도 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "353defb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/06 16:33:50 WARN Utils: Your hostname, minseok-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/09/06 16:33:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/06 16:33:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/09/06 16:33:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eb4ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [Row('MZ', 'CA'), Row('RX','CA')]\n",
    "authors_df = spark.createDataFrame(rows, ['Authors', 'State'])\n",
    "# authors_df.show()\n",
    "\n",
    "# 데이터프레임을 만들어 show하면 에러발생\n",
    "# 실제로는 파일에서 데이터 프레임을 읽어 들여야하는 상황이 일반적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd46338",
   "metadata": {},
   "source": [
    "# 자주 쓰이는 데이터 프레임 작업들"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c321da6",
   "metadata": {},
   "source": [
    "## DataFrameReader / DataFrameWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58966afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 읽을 파일은 28개의 컬럼과 4,380,660개 이상의 레코드가 있기 때문에\n",
    "# 스키마를 정의하는 것이 효과적이다.\n",
    "from pyspark.sql.types import *\n",
    "fire_schema = StructType([StructField('CalNumber', IntegerType(),True),\n",
    "                          StructField('UnitID', StringType(),True),\n",
    "                          StructField('IncidentNumber', IntegerType(),True),\n",
    "                          StructField('CallType', StringType(),True),\n",
    "                          StructField('CallDate', StringType(),True),\n",
    "                          StructField('WatchDate', StringType(),True),\n",
    "                          StructField('CallFinalDisposition', StringType(),True),\n",
    "                          StructField('AvailableDtTm', StringType(),True),\n",
    "                          StructField('Address', StringType(),True),\n",
    "                          StructField('City', StringType(),True),\n",
    "                          StructField('Zipcode', IntegerType(),True),\n",
    "                          StructField('Battalion', StringType(),True),\n",
    "                          StructField('StationArea', StringType(),True),\n",
    "                          StructField('Box', StringType(),True),\n",
    "                          StructField('OriginalPriority', StringType(),True),\n",
    "                          StructField('Priority', StringType(),True),\n",
    "                          StructField('FinalPriority', IntegerType(),True),\n",
    "                          StructField('ALSUnit', BooleanType(),True),\n",
    "                          StructField('CallTypeGroup', StringType(),True),\n",
    "                          StructField('NumAlarms', IntegerType(),True),\n",
    "                          StructField('UnitType', StringType(),True),\n",
    "                          StructField('UnitSequenceCallDispatch', IntegerType(),True),\n",
    "                          StructField('FirePreventionDistrict', StringType(),True),\n",
    "                          StructField('SupervisorDistrict', StringType(),True),\n",
    "                          StructField('Neighborhood', StringType(),True),\n",
    "                          StructField('Location', StringType(),True),\n",
    "                          StructField('RowID', StringType(),True),\n",
    "                          StructField('Delay', FloatType(),True)])\n",
    "\n",
    "# DataFrameReader 인터페이스로 CSV파일을 읽는다.\n",
    "sf_fire_file = 'sf-fire-calls.csv'\n",
    "fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbd1b10",
   "metadata": {},
   "source": [
    "DataFrameWriter의 기본포맷은 파케이이며 데이터 압축에 스내피(snappy) 압축을 쓴다.<br>\n",
    "만약 데이터 프레임이 파케이로 쓰여졌다면 스키마는 파케이 메타데이터의 일부로 보존될 수 있다. 이런 경우 데이터 프레임으로 읽어 들일 때 수동으로 스키마를 적용할 필요가 없다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "896296ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/06 16:34:55 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "23/09/06 16:34:59 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: CallNumber, UnitID, IncidentNumber, CallType, CallDate, WatchDate, CallFinalDisposition, AvailableDtTm, Address, City, Zipcode, Battalion, StationArea, Box, OriginalPriority, Priority, FinalPriority, ALSUnit, CallTypeGroup, NumAlarms, UnitType, UnitSequenceInCallDispatch, FirePreventionDistrict, SupervisorDistrict, Neighborhood, Location, RowID, Delay\n",
      " Schema: CalNumber, UnitID, IncidentNumber, CallType, CallDate, WatchDate, CallFinalDisposition, AvailableDtTm, Address, City, Zipcode, Battalion, StationArea, Box, OriginalPriority, Priority, FinalPriority, ALSUnit, CallTypeGroup, NumAlarms, UnitType, UnitSequenceCallDispatch, FirePreventionDistrict, SupervisorDistrict, Neighborhood, Location, RowID, Delay\n",
      "Expected: CalNumber but found: CallNumber\n",
      "CSV file: file:///home/minseok/spark-3.4.1-bin-hadoop3/python/Learning_Spark/Chapter3/sf-fire-calls.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 파케이로 저장\n",
    "parquet_path = './파케이저장연습'\n",
    "fire_df.write.format('parquet').save(parquet_path)\n",
    "\n",
    "# 폴더로 저장된다.\n",
    "# parquet_path는 폴더경로를 의미함\n",
    "# 폴더안에 .parquet파일이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd746b47-52ab-427b-844c-7bf43021e8db",
   "metadata": {},
   "source": [
    "하이브 메타스토어에 메타데이터로 등록되는 테이블로 저장할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12605c2f-57d3-4d57-8710-139e5a19375c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/06 16:39:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: CallNumber, UnitID, IncidentNumber, CallType, CallDate, WatchDate, CallFinalDisposition, AvailableDtTm, Address, City, Zipcode, Battalion, StationArea, Box, OriginalPriority, Priority, FinalPriority, ALSUnit, CallTypeGroup, NumAlarms, UnitType, UnitSequenceInCallDispatch, FirePreventionDistrict, SupervisorDistrict, Neighborhood, Location, RowID, Delay\n",
      " Schema: CalNumber, UnitID, IncidentNumber, CallType, CallDate, WatchDate, CallFinalDisposition, AvailableDtTm, Address, City, Zipcode, Battalion, StationArea, Box, OriginalPriority, Priority, FinalPriority, ALSUnit, CallTypeGroup, NumAlarms, UnitType, UnitSequenceCallDispatch, FirePreventionDistrict, SupervisorDistrict, Neighborhood, Location, RowID, Delay\n",
      "Expected: CalNumber but found: CallNumber\n",
      "CSV file: file:///home/minseok/spark-3.4.1-bin-hadoop3/python/Learning_Spark/Chapter3/sf-fire-calls.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquet_table = 'parquet_table'\n",
    "fire_df.write.format('parquet').saveAsTable(parquet_table)\n",
    "\n",
    "# 마찬가지로 parquet_table는 폴더이름을 의미함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffb23fa-3bf5-4296-8228-d9e7aa1c9904",
   "metadata": {},
   "source": [
    "## 프로젝션 / 필터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9afcab40-9cf4-4a3f-9c79-1a60aa84e2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------+\n",
      "|IncidentNumber|AvailableDtTm         |CallType      |\n",
      "+--------------+----------------------+--------------+\n",
      "|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n",
      "|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n",
      "|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n",
      "|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n",
      "|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "few_fire_df = (fire_df\n",
    "               .select('IncidentNumber', 'AvailableDtTm', 'CallType')\n",
    "               .where(col('CallType') != 'Medical Incident'))\n",
    "few_fire_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736c2640-36f9-4253-a87a-18b50908e782",
   "metadata": {},
   "source": [
    "화재 신고로 기록된 CallType 종류가 몇 가지인지 알고 싶다면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73f4bd3e-fb8f-4393-8543-7dffa1fee239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DistinctCallTypes|\n",
      "+-----------------+\n",
      "|               30|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# countDistinct()를 써서 신고 타입의 개수를 되돌려 준다.\n",
    "# pandas의 nunique()\n",
    "from pyspark.sql.functions import *\n",
    "(fire_df\n",
    " .select('CallType')\n",
    " .where(col('CallType').isNotNull())\n",
    " .agg(countDistinct('CallType').alias('DistinctCallTypes'))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788ca182-8146-492f-8a82-a4461c5c21dc",
   "metadata": {},
   "source": [
    "Null이 아닌 신고 타입의 목록?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0aed6100-7edc-404b-8e64-6127e90af040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|CallType                           |\n",
      "+-----------------------------------+\n",
      "|Elevator / Escalator Rescue        |\n",
      "|Marine Fire                        |\n",
      "|Aircraft Emergency                 |\n",
      "|Confined Space / Structure Collapse|\n",
      "|Administrative                     |\n",
      "|Alarms                             |\n",
      "|Odor (Strange / Unknown)           |\n",
      "|Citizen Assist / Service Call      |\n",
      "|HazMat                             |\n",
      "|Watercraft in Distress             |\n",
      "+-----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(fire_df\n",
    " .select('CallType')\n",
    " .where(col('CallType').isNotNull())\n",
    " .distinct()\n",
    " .show(10, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19018004-75cc-441c-9194-2e9c8e095d78",
   "metadata": {},
   "source": [
    "## 칼럼의 이름 변경 및 추가 삭제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df88e9a1-e1f2-4933-81c1-fa651ff2d768",
   "metadata": {},
   "source": [
    "일단은 StructField를 써서 스키마 내에서 원하는 칼럼 이름들을 지정하면 결과 데이터 프레임에서 원하는 대로 칼럼 이름이 출력된다. 하지만 이것은 데이터 소스의 칼럼 이름을 무시하고 원하는 이름으로 읽어 오는 경우이므로 '변경'과는 조금 의미가 다를 수 있다.<br>\n",
    "다른 방법으로는 withColumnRenamed() 함수로 변경할 수 있다. <br>\n",
    "'Delay'칼럼을 'ResponseDelayedinMins'로 바꾼 후 5분 이상 걸린 응답시간만 출력한다. <br>\n",
    "이 경우에도 데이터 프레임 원본을 유히한 채로 컬럼 이름이 변경된 새로운 데이터 프레임을 받아 온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53a0afc0-7b0e-41d0-8b76-b55a257ac05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|ResponseDelayedinMins|\n",
      "+---------------------+\n",
      "|5.35                 |\n",
      "|6.25                 |\n",
      "|5.2                  |\n",
      "|5.6                  |\n",
      "|7.25                 |\n",
      "+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_fire_df = fire_df.withColumnRenamed('Delay','ResponseDelayedinMins')\n",
    "(new_fire_df\n",
    " .select('ResponseDelayedinMins')\n",
    " .where(col('ResponseDelayedinMins') > 5)\n",
    " .show(5, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88790424-6424-4146-a9f5-2a71895def8e",
   "metadata": {},
   "source": [
    "날짜 타입으로 변경\n",
    "- to_timestamp\n",
    "- to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04194ed2-17e2-4f99-a419-a1dfc56f1319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|IncidentDate       |OnWatchDate        |AvailableDtTs      |\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 06:01:58|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_ts_df = (new_fire_df\n",
    "              .withColumn('IncidentDate', to_timestamp(col('CallDate'), 'MM/dd/yyyy'))\n",
    "              .drop('CallDate')\n",
    "              # 'CallDate' 칼럼값을 형식이 'MM/dd/yyyy'인 timestamp유형으로 바꿔 'IncidentDate' 컬럼으로 생성\n",
    "              # 기존 컬럼은 제거\n",
    "              .withColumn('OnWatchDate', to_timestamp(col('WatchDate'), 'MM/dd/yyyy'))\n",
    "              .drop('WatchDate')\n",
    "              .withColumn('AvailableDtTs', to_timestamp(col('AvailableDtTm'), 'MM/dd/yyyy hh:mm:ss a'))\n",
    "              .drop('AvailableDtTm'))\n",
    "\n",
    "# 변환된 칼럼들을 가져온다.\n",
    "(fire_ts_df\n",
    " .select('IncidentDate', 'OnWatchDate', 'AvailableDtTs')\n",
    " .show(5, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e21773-bfe6-4ca9-9050-98078ab75bf4",
   "metadata": {},
   "source": [
    "날짜/시간 칼럼을 가지게 되었으므로 이후에 탐색을 할 때는 pyspark.sql.functions에서 dayofmonth(), dayofyear(), dayofweek() 같은 함수들을 써서 질의할 수 있다.<br>\n",
    "몇 년 동안의 소방서 호출이 포함되어 있는지 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58521287-0954-4f9c-9073-f74a25833b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|year(IncidentDate)|\n",
      "+------------------+\n",
      "|              2000|\n",
      "|              2001|\n",
      "|              2002|\n",
      "|              2003|\n",
      "|              2004|\n",
      "|              2005|\n",
      "|              2006|\n",
      "|              2007|\n",
      "|              2008|\n",
      "|              2009|\n",
      "|              2010|\n",
      "|              2011|\n",
      "|              2012|\n",
      "|              2013|\n",
      "|              2014|\n",
      "|              2015|\n",
      "|              2016|\n",
      "|              2017|\n",
      "|              2018|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(fire_ts_df\n",
    " .select(year('IncidentDate'))\n",
    " # IncidentDate 칼럼의 연도를 가져온다.\n",
    " .distinct()\n",
    " .orderBy(year('IncidentDate'))\n",
    " # select(year('IncidentDate'))로 인해 칼럼이름이 year('IncidentDate')이다.\n",
    " # 따라서 orderBy 메소드 인자로 칼럼이름인 year('IncidentDate')를 넣었다.\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954cbd99-c578-43eb-822f-5b0a87a7355e",
   "metadata": {},
   "source": [
    "## 집계연산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dfda9d-1625-4592-88e6-0e712abcbfda",
   "metadata": {},
   "source": [
    "가장 흔한 형태의 신고는?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13eb0087-69f1-4f39-88bc-702c79d46dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------+\n",
      "|CallType                       |count |\n",
      "+-------------------------------+------+\n",
      "|Medical Incident               |113794|\n",
      "|Structure Fire                 |23319 |\n",
      "|Alarms                         |19406 |\n",
      "|Traffic Collision              |7013  |\n",
      "|Citizen Assist / Service Call  |2524  |\n",
      "|Other                          |2166  |\n",
      "|Outside Fire                   |2094  |\n",
      "|Vehicle Fire                   |854   |\n",
      "|Gas Leak (Natural and LP Gases)|764   |\n",
      "|Water Rescue                   |755   |\n",
      "+-------------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(fire_ts_df\n",
    " .select('CallType')\n",
    " .where(col('CallType').isNotNull())\n",
    " .groupBy('CallType')\n",
    " .count()\n",
    " .orderBy('count', ascending=False)\n",
    " .show(n=10, truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ebb26e3-bd5a-44db-b55e-a3bbbf84b717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/06 17:34:49 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: CallNumber, UnitID, IncidentNumber, CallType, CallDate, WatchDate, CallFinalDisposition, AvailableDtTm, Address, City, Zipcode, Battalion, StationArea, Box, OriginalPriority, Priority, FinalPriority, ALSUnit, CallTypeGroup, NumAlarms, UnitType, UnitSequenceInCallDispatch, FirePreventionDistrict, SupervisorDistrict, Neighborhood, Location, RowID, Delay\n",
      " Schema: CalNumber, UnitID, IncidentNumber, CallType, CallDate, WatchDate, CallFinalDisposition, AvailableDtTm, Address, City, Zipcode, Battalion, StationArea, Box, OriginalPriority, Priority, FinalPriority, ALSUnit, CallTypeGroup, NumAlarms, UnitType, UnitSequenceCallDispatch, FirePreventionDistrict, SupervisorDistrict, Neighborhood, Location, RowID, Delay\n",
      "Expected: CalNumber but found: CallNumber\n",
      "CSV file: file:///home/minseok/spark-3.4.1-bin-hadoop3/python/Learning_Spark/Chapter3/sf-fire-calls.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(CalNumber=20110016, UnitID='T13', IncidentNumber=2003235, CallType='Structure Fire', CallFinalDisposition='Other', Address='2000 Block of CALIFORNIA ST', City='SF', Zipcode=94109, Battalion='B04', StationArea='38', Box='3362', OriginalPriority='3', Priority='3', FinalPriority=3, ALSUnit=False, CallTypeGroup=None, NumAlarms=1, UnitType='TRUCK', UnitSequenceCallDispatch=2, FirePreventionDistrict='4', SupervisorDistrict='5', Neighborhood='Pacific Heights', Location='(37.7895840679362, -122.428071912459)', RowID='020110016-T13', ResponseDelayedinMins=2.950000047683716, IncidentDate=datetime.datetime(2002, 1, 11, 0, 0), OnWatchDate=datetime.datetime(2002, 1, 10, 0, 0), AvailableDtTs=datetime.datetime(2002, 1, 11, 1, 51, 44)),\n",
       " Row(CalNumber=20110022, UnitID='M17', IncidentNumber=2003241, CallType='Medical Incident', CallFinalDisposition='Other', Address='0 Block of SILVERVIEW DR', City='SF', Zipcode=94124, Battalion='B10', StationArea='42', Box='6495', OriginalPriority='3', Priority='3', FinalPriority=3, ALSUnit=True, CallTypeGroup=None, NumAlarms=1, UnitType='MEDIC', UnitSequenceCallDispatch=1, FirePreventionDistrict='10', SupervisorDistrict='10', Neighborhood='Bayview Hunters Point', Location='(37.7337623673897, -122.396113802632)', RowID='020110022-M17', ResponseDelayedinMins=4.699999809265137, IncidentDate=datetime.datetime(2002, 1, 11, 0, 0), OnWatchDate=datetime.datetime(2002, 1, 10, 0, 0), AvailableDtTs=datetime.datetime(2002, 1, 11, 3, 1, 18))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "데이터 프레임 API는 collect() 함수를 제공하지만 극단적으로 큰 데이터 프레임에서는 메모리 부족 예외(OOM)를 발생시킬 수 있기\n",
    "때문에 자원도 많이 쓰고 위험하다. collect()는 전체 데이터 프레임 혹은 데이터세트의 모든 Row 객체 모음을 되돌려 준다.\n",
    "(리스트 형태로 되돌려줌)\n",
    "몇 개의 Row 결과만 보고 싶다면 최초 n개의 Row 객체만 되돌려 주는 take(n) 함수를 쓰는 것이 훨씬 좋다.\n",
    "'''\n",
    "\n",
    "fire_ts_df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec83fdfd-1ca1-4d84-a58a-538b1c800c70",
   "metadata": {},
   "source": [
    "## 그 외 일반적인 데이터 프레임 연산들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebf19525-0c40-4fc5-a3ea-14ebc7180b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "|sum(NumAlarms)|avg(ResponseDelayedinMins)|min(ResponseDelayedinMins)|max(ResponseDelayedinMins)|\n",
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "|        176170|         3.892364154521585|               0.016666668|                   1844.55|\n",
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "(fire_ts_df\n",
    " .select(F.sum('NumAlarms'), F.avg('ResponseDelayedinMins'),\n",
    "         F.min('ResponseDelayedinMins'), F.max('ResponseDelayedinMins'))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037fd152-09dd-4f3c-b7de-1b1d2a22e554",
   "metadata": {},
   "source": [
    "stat(), describe(), correlation(), covariance(), sampleBy(), approxQuantile(), frequentItems() 등의 API 문서를 읽어보기 바람."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976c632e-512c-4935-b344-04807f377f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
