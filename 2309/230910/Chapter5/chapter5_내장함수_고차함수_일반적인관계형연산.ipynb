{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad815a4a-6286-4c4c-8a79-3db4080dc490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec0c63f3-44b1-4df5-8f52-62b6dca4e241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/10 15:43:09 WARN Utils: Your hostname, minseok-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/09/10 15:43:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/10 15:43:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('high_function').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386321fe-53f3-4f93-9f78-975ed2dde160",
   "metadata": {},
   "source": [
    "## 복잡한 데이터 유형을 위한 내장 함수\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/sql/index.html\n",
    "\n",
    "책에서는 array, map을 소개했다. 링크에서 array, map따로 찾아서 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5a957a-4ef5-4efa-8575-9676e5b5c19d",
   "metadata": {},
   "source": [
    "## 고차함수\n",
    "\n",
    "위의 내장함수 외에도 익명 람다 함수를 인수로 사용하는 고차 함수가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a0540-4bfa-44fc-b86e-c34128157e84",
   "metadata": {},
   "source": [
    "- transform() <br>\n",
    "\n",
    "배열과 익명함수를 입력하여 사용한다. 각 요소에 익명 함수를 적용한 다음, 결과를 출력 배열에 할당한다.(UDF 접근 방식과 유사하지만 더 효율적이다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ca71632-00b6-41a8-aae9-85df74331d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f1b4001-2cfb-493e-983c-e0ae6a15896f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             celsius|\n",
      "+--------------------+\n",
      "|[35, 36, 32, 30, ...|\n",
      "|[31, 32, 34, 55, 56]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([StructField('celsius', ArrayType(IntegerType()))])\n",
    "\n",
    "t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]\n",
    "t_c = spark.createDataFrame(t_list, schema)\n",
    "t_c.createOrReplaceTempView('tc')\n",
    "\n",
    "# 데이터 프레임 출력\n",
    "t_c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f47824ed-9dc4-49b0-be6d-69c403cf95b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             celsius|          fagrenheit|\n",
      "+--------------------+--------------------+\n",
      "|[35, 36, 32, 30, ...|[95, 96, 89, 86, ...|\n",
      "|[31, 32, 34, 55, 56]|[87, 89, 93, 131,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 온도의 배열에 대해 섭씨를 화씨로 계산\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius, transform(celsius, t->((t*9) div 5 + 32)) AS fagrenheit\n",
    "FROM tc\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8967163d-c7a3-48ec-b40b-a5373ce775c3",
   "metadata": {},
   "source": [
    "- filter() <br>\n",
    "\n",
    "입력한 배열의 요소 중 불린 함수가 참인 요소만으로 구성된 배열을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "94ece686-b768-4d65-a519-7a0a697ad950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             celsius|    high|\n",
      "+--------------------+--------+\n",
      "|[35, 36, 32, 30, ...|[40, 42]|\n",
      "|[31, 32, 34, 55, 56]|[55, 56]|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 온도의 배열에 대해 섭씨 38도 이상을 필터\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius, filter(celsius, t-> t > 38) AS high\n",
    "FROM tc\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38862a5c-36da-4176-96db-c736927b1f74",
   "metadata": {},
   "source": [
    "- exists() <br>\n",
    "\n",
    "입력한 배열의 요소 중 불린 함수를 만족시키는 것이 존재하면 참을 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7030bacf-8d21-40f3-afd9-2aa6c01cf14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             celsius|threshold|\n",
      "+--------------------+---------+\n",
      "|[35, 36, 32, 30, ...|     true|\n",
      "|[31, 32, 34, 55, 56]|    false|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 온도의 배열에 섭씨 38도의 온도가 있는가?\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius, exists(celsius, t-> t = 38) AS threshold\n",
    "FROM tc\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6d9a47-d99c-41ef-bf54-ee4ceb6869f8",
   "metadata": {},
   "source": [
    "- reduce() <br>\n",
    "\n",
    "`reduce(array<T>, B, function<B, T, B>, function<B, R>`\n",
    "\n",
    "reduce(expr, start, merge, finish) - Applies a binary operator to an initial state and all elements in the array, and reduces this to a single state. The final state is converted into the final result by applying a finish function.\n",
    "\n",
    "예제를 보고 이해하는 것이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a9e642b4-4a44-4a3c-b509-57f9cc922221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|reduce(array(1, 2, 3), 0, lambdafunction((namedlambdavariable() + namedlambdavariable()), namedlambdavariable(), namedlambdavariable()), lambdafunction(namedlambdavariable(), namedlambdavariable()))|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                                                                                                     6|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT reduce(array(1,2,3), 0, (acc, x) -> acc + x)\"\"\").show()\n",
    "\n",
    "# 0: an initial state\n",
    "# acc: accumulator\n",
    "# 0 + 1 = 1\n",
    "# 1 + 2 = 3\n",
    "# 3 + 3 = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "20d2b1aa-d013-49fc-83e7-2f4169b2c367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|reduce(array(1, 2, 3), 0, lambdafunction((namedlambdavariable() + namedlambdavariable()), namedlambdavariable(), namedlambdavariable()), lambdafunction((namedlambdavariable() * 10), namedlambdavariable()))|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                                                                                                           60|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT reduce(array(1,2,3), 0, (acc, x) -> acc + x, acc -> acc * 10)\"\"\").show()\n",
    "\n",
    "# 여기서 acc는 이해를 위한 이름이다.\n",
    "# 어떠한 단어든 사용가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d6ce30-76a1-4ee7-a1af-3acec9490c08",
   "metadata": {},
   "source": [
    "# 일반적인 데이터 프레임 및 스파크 SQL 작업\n",
    "\n",
    "스파크 SQL의 기능 중 일부는 데이터 프레임의 다양한 기능에서 유래된다. 작업 목록은 매우 광범위하며 다음을 포함한다.\n",
    "\n",
    "- 집계 함수\n",
    "- 수집 함수\n",
    "- 날짜/시간 함수\n",
    "- 수학 함수\n",
    "- 기타 함수\n",
    "- 비집계 함수\n",
    "- 정렬 함수\n",
    "- 문자열 함수\n",
    "- UDF 함수\n",
    "- 윈도우 함수\n",
    "- 참고 - https://spark.apache.org/docs/latest/api/sql/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472bfce6-0d34-4eb1-ac2d-fd04cda45e64",
   "metadata": {},
   "source": [
    "데이터 준비\n",
    "1. 공항(airportsna) 정보 데이터, 미국 비행 지연(departureDelays) 데이터를 가져온다.\n",
    "2. expr()을 사용하여 delay 및 distance 칼럼을 STRING에서 INT로 변환한다.\n",
    "3. 작은 테이블 foo를 만든다. 작은 시간 범위 동안 시애틀(SEA)에서 출발하여 샌프란시스코(SFO)에 도착하는 3개의 항공편에 대한 정보만 포함되어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5191965b-3f90-4bd6-be45-2366323fe887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "508e468e-d2c9-4a60-acc9-9625bb613b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로 설정\n",
    "tripdelaysFilePath = 'departuredelays.csv'\n",
    "airportsnaFilePath = 'airport-codes-na.txt'\n",
    "\n",
    "# 공항 데이터세트를 읽어 오기\n",
    "airportsna = (spark.read\n",
    "              .format('csv')\n",
    "              .options(header = 'true', inferShema='true', sep='\\t')\n",
    "              .load(airportsnaFilePath))\n",
    "\n",
    "airportsna.createOrReplaceTempView('airports_na')\n",
    "\n",
    "# 출발 지연 데이터세트를 읽어 오기\n",
    "departureDelays = (spark.read\n",
    "                   .format('csv')\n",
    "                   .options(header='true')\n",
    "                   .load(tripdelaysFilePath))\n",
    "\n",
    "departureDelays = (departureDelays\n",
    "                   .withColumn('delay', expr('CAST(delay as INT) as delay'))\n",
    "                   .withColumn('distance', expr('CAST(distance as INT) as distance')))\n",
    "\n",
    "departureDelays.createOrReplaceTempView('departureDelays')\n",
    "\n",
    "# 임시 작은 테이블 생성\n",
    "foo = (departureDelays\n",
    "       .filter(expr(\"\"\"origin == 'SEA' AND destination == 'SFO' and\n",
    "       date like '01010%' and delay > 0\"\"\")))\n",
    "foo.createOrReplaceTempView('foo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0cd624dd-3694-4052-8c55-914aff1cb5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "+-----------+-----+-------+----+\n",
      "\n",
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM airports_na LIMIT 10').show()\n",
    "\n",
    "spark.sql('SELECT * FROM departureDelays LIMIT 10').show()\n",
    "\n",
    "spark.sql('SELECT * FROM foo').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162eb58c-77e6-48ec-b638-91b7af63d644",
   "metadata": {},
   "source": [
    "- Union\n",
    "\n",
    "동일한 스키마를 가진 두 개의 서로 다른 데이터 프레임을 함께 결합한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3b88c8e4-e0f5-4808-93c3-267e339dd0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 두 테이블 결합\n",
    "bar = departureDelays.union(foo)\n",
    "bar.createOrReplaceTempView('bar')\n",
    "\n",
    "# 결합된 결과 보기(특정 시간 범위에 대한 SEA와 SFO를 필터)\n",
    "bar.filter(expr(\"\"\"origin == 'SEA' AND destination == 'SFO'\n",
    "AND date LIKE '01010%' AND delay > 0\"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc04c63c-9f4b-4d35-acd0-ec38a1f9ac95",
   "metadata": {},
   "source": [
    "bar 데이터 프레임은 foo와 delays의 결합이다.(pandas의 concat!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4ea779ae-5aef-4f6d-90a7-09969f204256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# In SQL\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM bar\n",
    "WHERE origin = 'SEA'\n",
    "AND destination = 'SFO'\n",
    "AND date LIKE '01010%'\n",
    "AND delay > 0\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaeca29-9f0c-4245-8efe-7b7f4c0cee3a",
   "metadata": {},
   "source": [
    "- join\n",
    "\n",
    "기본적으로 스파크 SQL 조인은 inner join이며 옵션은 inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi 및 left_anti이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "84ccddc4-4121-4b43-adac-6f1e0a0e26bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 출발 지연 데이터(foo)와 공항 정보의 조인\n",
    "foo.join(airportsna,\n",
    "         airportsna.IATA == foo.origin)\\\n",
    ".select('City', 'State', 'date', 'delay', 'distance', 'destination').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "58aedd2c-2be8-4998-8ad8-c3be84c952a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   city|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# SQL 예제\n",
    "spark.sql(\"\"\"\n",
    "SELECT a.city, a.State, f.date, f.delay, f.distance, f.destination\n",
    "FROM foo f\n",
    "JOIN airports_na a\n",
    "ON a.IATA = f.origin\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cb2110-f1f1-4081-9bcc-461a0d58379c",
   "metadata": {},
   "source": [
    "- 윈도우\n",
    "\n",
    "윈도우 함수는 일반적으로 윈도우(입력 행의 범위) 행의 값을 사용하여 다른 행의 형태로 값 집합을 반환한다. 윈도우 함수를 사용하면 모든 입력 행에 대해 단일값을 반환하면서 행 그룹에 대해 작업할 수 있다. 이 섹션에서는 dense_rank()을 사용한다.\n",
    "\n",
    "https://www.databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a5af92-4e12-47ad-b9f5-623bd51fd449",
   "metadata": {},
   "source": [
    "다음 쿼리에서는 시애틀SEA, 샌프란시스코SFO 및 뉴욕JFK에서 출발하여 특정 목적지 위치로 이동하는 항공편에서 기록된 TotalDelay(sum(Delay)로 계산되는)에 대한 검토부터 시작한다.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7fae89-4d04-4223-92c6-98399af95a89",
   "metadata": {},
   "source": [
    "이러한 각 출발 공항에 대해 가장 많은 지연이 발생한 3개의 목적지를 찾으려면 어떻게 해야 하는가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6141b457-8c21-4abf-8d15-8472147859f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL 예제\n",
    "spark.sql(\"\"\"\n",
    "SELECT origin, destination, TotalDelays, rank\n",
    "FROM (\n",
    "SELECT origin, destination, TotalDelays,\n",
    "dense_rank() OVER (PARTITION BY origin ORDER BY TotalDelays DESC) AS rank\n",
    "FROM departureDelaysWindow\n",
    ") t\n",
    "WHERE rank <= 3\n",
    "\"\"\").show()\n",
    "\n",
    "# departureDelaysWindow 테이블을 만드는 과정에서 에러가 발생했음. 그냥 코드만 남겨놓는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a83dc2c-4a5a-4feb-9ee9-a55e7b392272",
   "metadata": {},
   "source": [
    "각 윈도우 그룹은 단일 이그제큐터에서 실행될 수 있어야 하며 실행 중에는 단일 파티션으로 구성된다는 점에 유의해야 한다. 그러므로 쿼리가 제한되지 않는지 확인해야 한다.(예: 윈도우 크기 제한)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c25c3-ab98-4e5d-b69a-a6293ec4b650",
   "metadata": {},
   "source": [
    "- 수정\n",
    "\n",
    "데이터 프레임을 수정하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b11379-f169-415a-b377-5eb05aa1b097",
   "metadata": {},
   "source": [
    "열추가 - withColumn() 함수를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "033c700c-edb4-46b3-9238-ba48df012aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "foo2 = (foo.withColumn(\n",
    "    'status', expr(\"CASE WHEN delay <= 10 THEN 'On-time' ELSE 'Delayed' END\")\n",
    "                  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "17ac6a12-6110-4356-ac13-798f89f8c205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 74:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+-------+\n",
      "|    date|delay|distance|origin|destination| status|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "|01010710|   31|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|  104|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|    5|     590|   SEA|        SFO|On-time|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "foo2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5577c99c-7370-4b10-ba2a-e7391c0008a5",
   "metadata": {},
   "source": [
    "열 삭제 - drop() 함수를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3834956a-b96f-4fec-af48-2d1c1a466d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------+\n",
      "|    date|distance|origin|destination| status|\n",
      "+--------+--------+------+-----------+-------+\n",
      "|01010710|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|     590|   SEA|        SFO|On-time|\n",
      "+--------+--------+------+-----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "foo3 = foo2.drop('delay')\n",
    "foo3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778ec493-daaf-4f89-a76a-b4792925db77",
   "metadata": {},
   "source": [
    "컬럼명 바꾸기 - withColumnRenamed() 함수를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "74042a24-f1b8-47d1-985a-7f13012a8150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------------+\n",
      "|    date|distance|origin|destination|flight_status|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "|01010710|     590|   SEA|        SFO|      Delayed|\n",
      "|01010955|     590|   SEA|        SFO|      Delayed|\n",
      "|01010730|     590|   SEA|        SFO|      On-time|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "foo4 = foo3.withColumnRenamed('status', 'flight_status')\n",
    "foo4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d259f8-b8b4-43ea-8546-c6a53e1ba224",
   "metadata": {},
   "source": [
    "피벗\n",
    "\n",
    "개념 설명을 위해 몇 가지 데이터를 살펴본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "216e8e6f-181a-4812-9dba-9db8a08eaba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 79:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----+\n",
      "|destination|month|delay|\n",
      "+-----------+-----+-----+\n",
      "|        ORD|    1|   92|\n",
      "|        JFK|    1|   -7|\n",
      "|        DFW|    1|   -5|\n",
      "|        MIA|    1|   -3|\n",
      "|        DFW|    1|   -3|\n",
      "|        DFW|    1|    1|\n",
      "|        ORD|    1|  -10|\n",
      "|        DFW|    1|   -6|\n",
      "|        DFW|    1|   -2|\n",
      "|        ORD|    1|   -3|\n",
      "|        ORD|    1|    0|\n",
      "|        DFW|    1|   23|\n",
      "|        DFW|    1|   36|\n",
      "|        ORD|    1|  298|\n",
      "|        JFK|    1|    4|\n",
      "|        DFW|    1|    0|\n",
      "|        MIA|    1|    2|\n",
      "|        DFW|    1|    0|\n",
      "|        DFW|    1|    0|\n",
      "|        ORD|    1|   83|\n",
      "+-----------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay\n",
    "FROM departureDelays\n",
    "WHERE origin = 'SEA'\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963525d8-5c30-4371-8a5a-819a78470f17",
   "metadata": {},
   "source": [
    "피벗을 사용하면 month 칼럼에 이름을 배치할 수 있을 뿐만 아니라 (1과 2 대신 각각 Jan과 Feb를 표시할 수 있음) 목적지 및 월별 지연에 대한 집계 계산(이 경우 평균 및 최대)을 수행할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "26f5a206-7b36-45e6-9dad-df854619a1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 80:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+------------+\n",
      "|destination|JAN_AvgDelay|JAN_MaxDelay|FEB_AvgDelay|FEB_MaxDelay|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "|        ABQ|       19.86|         316|       11.42|          69|\n",
      "|        ANC|        4.44|         149|        7.90|         141|\n",
      "|        ATL|       11.98|         397|        7.73|         145|\n",
      "|        AUS|        3.48|          50|       -0.21|          18|\n",
      "|        BOS|        7.84|         110|       14.58|         152|\n",
      "|        BUR|       -2.03|          56|       -1.89|          78|\n",
      "|        CLE|       16.00|          27|        null|        null|\n",
      "|        CLT|        2.53|          41|       12.96|         228|\n",
      "|        COS|        5.32|          82|       12.18|         203|\n",
      "|        CVG|       -0.50|           4|        null|        null|\n",
      "|        DCA|       -1.15|          50|        0.07|          34|\n",
      "|        DEN|       13.13|         425|       12.95|         625|\n",
      "|        DFW|        7.95|         247|       12.57|         356|\n",
      "|        DTW|        9.18|         107|        3.47|          77|\n",
      "|        EWR|        9.63|         236|        5.20|         212|\n",
      "|        FAI|        1.84|         160|        4.21|          60|\n",
      "|        FAT|        1.36|         119|        5.22|         232|\n",
      "|        FLL|        2.94|          54|        3.50|          40|\n",
      "|        GEG|        2.28|          63|        2.87|          60|\n",
      "|        HDN|       -0.44|          27|       -6.50|           0|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM (\n",
    "SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay\n",
    "FROM departureDelays WHERE origin = 'SEA'\n",
    ")\n",
    "PIVOT (\n",
    "CAST(AVG(delay) AS DECIMAL(4,2)) AS AvgDelay, MAX(delay) AS MaxDelay\n",
    "For month IN (1 JAN, 2 FEB)\n",
    ")\n",
    "ORDER BY destination\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b189535a-9fa2-4271-9443-63a00289486f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
