{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Code Along\n",
    "This is a code along of the famous titanic dataset, its always nice to start off with this dataset because it is an example you will find across pretty much every data analysis language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('myproj').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('../titanic.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId',\n",
       " 'Survived',\n",
       " 'Pclass',\n",
       " 'Name',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Ticket',\n",
       " 'Fare',\n",
       " 'Cabin',\n",
       " 'Embarked']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 컬럼만 가져옴\n",
    "my_cols = data.select(['Survived',\n",
    " 'Pclass',\n",
    " 'Sex',\n",
    " 'Age',\n",
    " 'SibSp',\n",
    " 'Parch',\n",
    " 'Fare',\n",
    " 'Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측값은 삭제했음\n",
    "my_final_data = my_cols.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Categorical Columns\n",
    "\n",
    "Let's break this down into multiple steps to make it all clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,\n",
    "                                OneHotEncoder,StringIndexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 단계를 거쳐 원-핫 인코딩한다.\n",
    "# 1. 문자열을 숫자로\n",
    "# 2, 바뀐 숫자들을 원-핫 인코딩\n",
    "# 결과는 벡터 형식으로 출력된다.\n",
    "gender_indexer = StringIndexer(inputCol='Sex',outputCol='SexIndex')\n",
    "gender_encoder = OneHotEncoder(inputCol='SexIndex',outputCol='SexVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embark_indexer = StringIndexer(inputCol='Embarked',outputCol='EmbarkIndex')\n",
    "embark_encoder = OneHotEncoder(inputCol='EmbarkIndex',outputCol='EmbarkVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['Pclass',\n",
    " 'SexVec',\n",
    " 'Age',\n",
    " 'SibSp',\n",
    " 'Parch',\n",
    " 'Fare',\n",
    " 'EmbarkVec'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines \n",
    "\n",
    "Let's see an example of how to use pipelines (we'll get a lot more practice with these later!)\n",
    "\n",
    "각 작업의 진행을 위해 단계를 생성하는 역할을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_titanic = LogisticRegression(featuresCol='features',labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인은 작업에 필요한 요소를 모두 포함하는 'stages'라는 인자를 갖는다.\n",
    "# (인자에는 앞에서 만들어 놓은 객체를 저장한 변수를 리스트에 나열한다.)\n",
    "# 변환을 위해 단계를 설계하고 집합시킨 다음 모델에게 넘겨주는 것이라 볼 수 있다.\n",
    "# (리스트로 나열하는 것이 단계를 설계하는 것으로 보이고, 집합시키고 모델에\n",
    "# 넘겨주는 것은 파이프라인 객체가 하는 것으로 보인다.)\n",
    "pipeline = Pipeline(stages=[gender_indexer,embark_indexer,\n",
    "                           gender_encoder,embark_encoder,\n",
    "                           assembler,log_reg_titanic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_titanic_data, test_titanic_data = my_final_data.randomSplit([0.7,.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 객체의 장점은 일반적인 모델처럼 사용할 수 있다는 것이다.\n",
    "fit_model = pipeline.fit(train_titanic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터를 변환\n",
    "# 결과물을 테스트 데이터 셋에서 시험해보는거라함\n",
    "# (이전까지는 fit -> evaluate 였고, 배포단계에서 transform을 사용했는데.. 헷갈린다.)\n",
    "# (evaluate은 LinearRegressionSummary객체를 반환한다. 속성으로 평가지표를 갖는다.)\n",
    "# (accuracy, weightedPrecision, weightedRecall, ... )\n",
    "# (여태 사용한 predictions도 있는데 Dataframe outputted by the model's transform method을 의미한다. )\n",
    "results = fit_model.transform(test_titanic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-----+-----+--------+--------+--------+-----------+-------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|Survived|Pclass|   Sex| Age|SibSp|Parch|    Fare|Embarked|SexIndex|EmbarkIndex|       SexVec|    EmbarkVec|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+------+------+----+-----+-----+--------+--------+--------+-----------+-------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|       0|     1|female| 2.0|    1|    2|  151.55|       S|     1.0|        0.0|    (1,[],[])|(2,[0],[1.0])|[1.0,0.0,2.0,1.0,...|[-4.0662102912122...|[0.01685332603877...|       1.0|\n",
      "|       0|     1|female|50.0|    0|    0| 28.7125|       C|     1.0|        1.0|    (1,[],[])|(2,[1],[1.0])|(8,[0,2,5,7],[1.0...|[-2.5347019977799...|[0.07346096513819...|       1.0|\n",
      "|       0|     1|  male|18.0|    1|    0|   108.9|       C|     0.0|        1.0|(1,[0],[1.0])|(2,[1],[1.0])|[1.0,1.0,18.0,1.0...|[-0.8745043699613...|[0.29431790148663...|       1.0|\n",
      "|       0|     1|  male|19.0|    3|    2|   263.0|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,19.0,3.0...|[-0.1545074092321...|[0.46144980811075...|       1.0|\n",
      "|       0|     1|  male|28.0|    1|    0| 82.1708|       C|     0.0|        1.0|(1,[0],[1.0])|(2,[1],[1.0])|[1.0,1.0,28.0,1.0...|[-0.4013551815775...|[0.40098678654035...|       1.0|\n",
      "|       0|     1|  male|29.0|    0|    0|    30.0|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,29.0,0.0...|[-0.1894353283472...|[0.45278228709370...|       1.0|\n",
      "|       0|     1|  male|36.0|    1|    0|   78.85|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,36.0,1.0...|[0.29543544426220...|[0.57332629587188...|       0.0|\n",
      "|       0|     1|  male|38.0|    0|    1|153.4625|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,38.0,0.0...|[-0.0795975416645...|[0.48011111442330...|       1.0|\n",
      "|       0|     1|  male|39.0|    0|    0|     0.0|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|(8,[0,1,2,6],[1.0...|[0.29133479064503...|[0.57232288033248...|       0.0|\n",
      "|       0|     1|  male|40.0|    0|    0|     0.0|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|(8,[0,1,2,6],[1.0...|[0.33242183317129...|[0.58234852921446...|       0.0|\n",
      "|       0|     1|  male|46.0|    0|    0|    79.2|       C|     0.0|        1.0|(1,[0],[1.0])|(2,[1],[1.0])|[1.0,1.0,46.0,0.0...|[0.03405204135113...|[0.50851218783407...|       0.0|\n",
      "|       0|     1|  male|46.0|    1|    0|  61.175|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,46.0,1.0...|[0.74748843908054...|[0.67863119652614...|       0.0|\n",
      "|       0|     1|  male|47.0|    0|    0| 34.0208|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,47.0,0.0...|[0.54076301417398...|[0.63198989642107...|       0.0|\n",
      "|       0|     1|  male|47.0|    0|    0|    52.0|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,47.0,0.0...|[0.49887166172389...|[0.62219413090228...|       0.0|\n",
      "|       0|     1|  male|49.0|    1|    1|110.8833|       C|     0.0|        1.0|(1,[0],[1.0])|(2,[1],[1.0])|[1.0,1.0,49.0,1.0...|[0.4222936481163,...|[0.60403196923245...|       0.0|\n",
      "|       0|     1|  male|50.0|    1|    0|    55.9|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,50.0,1.0...|[0.92412730533305...|[0.71588232568588...|       0.0|\n",
      "|       0|     1|  male|58.0|    0|    0|    29.7|       C|     0.0|        1.0|(1,[0],[1.0])|(2,[1],[1.0])|[1.0,1.0,58.0,0.0...|[0.64243104632013...|[0.65530279501164...|       0.0|\n",
      "|       0|     1|  male|61.0|    0|    0|    33.5|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,61.0,0.0...|[1.11719506822485...|[0.75346805982056...|       0.0|\n",
      "|       0|     1|  male|65.0|    0|    0|   26.55|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,65.0,0.0...|[1.29773666737727...|[0.78545382144259...|       0.0|\n",
      "|       0|     1|  male|65.0|    0|    1| 61.9792|       C|     0.0|        1.0|(1,[0],[1.0])|(2,[1],[1.0])|[1.0,1.0,65.0,0.0...|[0.88255090605870...|[0.70735055247608...|       0.0|\n",
      "+--------+------+------+----+-----+-----+--------+--------+--------+-----------+-------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()\n",
    "# test_titanic_data에 파이프라인이 모두 적용되고 예측값까지 추가됨\n",
    "# 그냥 이걸 transform이 하는일이라고 생각하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.show()에서 prediction 컬럼이 추가된 것을 보았다.\n",
    "# 따라서 rawPredictionCol에 'prediction'을 넣었다.\n",
    "# transform하면 항상 prediction 컬럼이 생성된다고 함\n",
    "\n",
    "my_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
    "                                       labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC = my_eval.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7599624060150376"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great Job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
