{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Consulting Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Binary Customer Churn\n",
    "\n",
    "A marketing agency has many customers that use their service to produce ads for the client/customer websites. They've noticed that they have quite a bit of churn in clients. They basically randomly assign account managers right now, but want you to create a machine learning model that will help predict which customers will churn (stop buying their service) so that they can correctly assign the customers most at risk to churn an account manager. Luckily they have some historical data, can you help them out? Create a classification algorithm that will help classify whether or not a customer churned. Then the company can test this against incoming data for future customers to predict which customers will churn and assign them an account manager.\n",
    "\n",
    "The data is saved as customer_churn.csv. Here are the fields and their definitions:\n",
    "\n",
    "    Name : Name of the latest contact at Company\n",
    "    Age: Customer Age\n",
    "    Total_Purchase: Total Ads Purchased\n",
    "    Account_Manager: Binary 0=No manager, 1= Account manager assigned\n",
    "    Years: Totaly Years as a customer\n",
    "    Num_sites: Number of websites that use the service.\n",
    "    Onboard_date: Date that the name of the latest contact was onboarded\n",
    "    Location: Client HQ Address\n",
    "    Company: Name of Client Company\n",
    "    \n",
    "Once you've created the model and evaluated it, test out the model on some new data (you can think of this almost like a hold-out set) that your client has provided, saved under new_customers.csv. The client wants to know which customers are most likely to churn given this data (they don't have the label yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('churn').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('../customer_churn.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Names',\n",
       " 'Age',\n",
       " 'Total_Purchase',\n",
       " 'Account_Manager',\n",
       " 'Years',\n",
       " 'Num_Sites',\n",
       " 'Onboard_date',\n",
       " 'Location',\n",
       " 'Company',\n",
       " 'Churn']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 컬럼만 가져옴\n",
    "my_cols = data.select([ # 'Names',\n",
    " # 'Age', # client의 나이는 영향을 주지 않을 것으로 예상\n",
    " 'Total_Purchase',\n",
    " # 'Account_Manager', 무작위로 배정되기 때문에 제외한다.\n",
    " 'Years',\n",
    " 'Num_Sites',\n",
    " 'Onboard_date',\n",
    " # 'Location', 웹 광고인데 회사 위치는 중요하지 않을 것이다.\n",
    " # 'Company', 회사 이름은 영향을 끼치지 않을 것이다.\n",
    " 'Churn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+---------+------------+-----+\n",
      "|Total_Purchase|Years|Num_Sites|Onboard_date|Churn|\n",
      "+--------------+-----+---------+------------+-----+\n",
      "|             0|    0|        0|           0|    0|\n",
      "+--------------+-----+---------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col, isnull\n",
    "\n",
    "# 결측값 확인\n",
    "my_cols.select([count(when(isnull(c), c)).alias(c) for c in my_cols.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+---------+-------------------+-----+\n",
      "|Total_Purchase|Years|Num_Sites|       Onboard_date|Churn|\n",
      "+--------------+-----+---------+-------------------+-----+\n",
      "|       11066.8| 7.22|      8.0|2013-08-30 07:00:40|    1|\n",
      "|      11916.22|  6.5|     11.0|2013-08-13 00:38:46|    1|\n",
      "|      12884.75| 6.67|     12.0|2016-06-29 06:20:07|    1|\n",
      "|       8010.76| 6.71|     10.0|2014-04-22 12:43:12|    1|\n",
      "|       9191.58| 5.56|      9.0|2016-01-19 15:31:15|    1|\n",
      "|      10356.02| 5.12|      8.0|2009-03-03 23:13:37|    1|\n",
      "|      11331.58| 5.23|     11.0|2016-12-05 03:35:43|    1|\n",
      "|       9885.12| 6.92|      9.0|2006-03-09 14:50:20|    1|\n",
      "|       14062.6| 5.46|     11.0|2011-09-29 05:47:23|    1|\n",
      "|       8066.94| 7.11|     11.0|2006-03-28 15:42:45|    1|\n",
      "|      11575.37| 5.22|      8.0|2016-11-13 13:13:01|    1|\n",
      "|       8771.02| 6.64|     11.0|2015-05-28 12:14:03|    1|\n",
      "|       8988.67| 4.84|     11.0|2011-02-16 08:10:47|    1|\n",
      "|       8283.32|  5.1|     13.0|2012-11-22 05:35:03|    1|\n",
      "|       6569.87|  4.3|     11.0|2015-03-28 02:13:44|    1|\n",
      "|      10494.82| 6.81|     12.0|2015-07-22 08:38:40|    1|\n",
      "|       8213.41| 7.35|     11.0|2006-09-03 06:13:55|    1|\n",
      "|      11226.88| 8.08|     12.0|2006-10-22 04:42:38|    1|\n",
      "|       5515.09| 6.85|      8.0|2015-10-07 00:27:10|    1|\n",
      "|        8046.4| 5.69|      8.0|2014-11-06 23:47:14|    1|\n",
      "+--------------+-----+---------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_cols.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VectorAssembler가 timestamp 유형은 지원하지 않아서 date 유형으로 바꿈\n",
    "# 근데 date 유형도 지원안함\n",
    "# 빼고 진행한다.\n",
    "# my_cols = my_cols.withColumn('trans_Onboard_date', my_cols['Onboard_date'].cast('date'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문자열이 없어 인덱서나 인코딩할 필요없다.\n",
    "굳이 파이프라인을 사용하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=['Total_Purchase',\n",
    " 'Years',\n",
    " 'Num_Sites',\n",
    "#  'trans_Onboard_date',],\n",
    "    ],\n",
    "    outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_cols = assembler.transform(my_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(featuresCol='features',labelCol='Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = final_cols.randomSplit([0.7,.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_log_reg = log_reg.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fit_log_reg.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
    "                                       labelCol='Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7320165719240774"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC = my_eval.evaluate(results)\n",
    "AUC"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
