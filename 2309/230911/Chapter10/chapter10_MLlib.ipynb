{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86ef3ac0-2387-43c7-96b6-4a99fe0a6299",
   "metadata": {},
   "source": [
    "# 머신러닝 파이프라인 설계\n",
    "Inside Airbnb의 샌프란시스코 주택 데이터 세트를 사용한다. 샌프란시스코의 에어비앤비 임대에 대한 정보가 포함되어 있으며 우리의 목표는 해당 도시의 숙소에 대한 1박 임대 가격을 예측하는 모델을 구축하는 것이다. <br>\n",
    "이 장의 목적은 MLlib을 사용하여 종단 간 파이프라인을 구축하는 데 필요한 기술과 지식을 갖추는 것이다. <br>\n",
    "\n",
    "- 변환기(transformer): 데이터 프레임을 입력으로 받아들이고, 하나 이상의 열이 추가된 새 데이터 프레임을 반환한다. 변환기는 데이터에서 매개변수를 학습하지 않고, 단순히 규칙 기반 변환을 적용하여 모델 훈련을 위한 데이터를 준비하거나 훈련된 MLlib모델을 사용하여 예측을 생성한다. .transform() 메서드가 있다.\n",
    "- 추정기(estimator): .fit()메서드를 통해 데이터 프레임에서 매개변수를 학습하고 변환기인 Model을 반환한다.\n",
    "- 파이프라인: 일련의 변환기와 추정기를 단일 모델로 구성한다. 파이프라인 자체가 추정기인 반면, pipeline.fit()의 출력은 변환기인 PipelineModel을 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273bebbd-cbc5-4b2c-9898-de527ee96e23",
   "metadata": {},
   "source": [
    "## 데이터 수집 및 탐색\n",
    "예시 데이터 세트의 데이터를 약간 사전 처리하여 이상값($0/1박)을 제거하고, 모든 정수를 두 배로 변환하고, 100개 이상의 필드에서 유익한 하위 집합을 선택했다. 또한 데이터 열에서 누락된 숫자값에 대해 중앙값을 입력하고 열을 추가했다.(bedrooms_na와 같이 열 이름 뒤에 _na가 옴)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18bd4f2c-df13-40dd-a4cc-6c3dc8c636ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/11 15:06:01 WARN Utils: Your hostname, minseok-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/09/11 15:06:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/11 15:06:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('airbnb_ml').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6acca2b3-a55b-409a-9332-38e42839fba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filePath = \"\"\"sf-airbnb-clean.parquet/\"\"\"\n",
    "airbnbDF = spark.read.parquet(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74e01ccc-d1c2-44d3-ae58-7b29b03c49a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|neighbourhood_cleansed|      room_type|bedrooms|bathrooms|number_of_reviews|price|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|      Western Addition|Entire home/apt|     1.0|      1.0|            180.0|170.0|\n",
      "|        Bernal Heights|Entire home/apt|     2.0|      1.0|            111.0|235.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|             17.0| 65.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|              8.0| 65.0|\n",
      "|      Western Addition|Entire home/apt|     2.0|      1.5|             27.0|785.0|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "airbnbDF.select(\"neighbourhood_cleansed\", 'room_type', 'bedrooms', 'bathrooms',\n",
    "                'number_of_reviews', 'price').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f21f14-c862-42e3-a7cc-62317045eea2",
   "metadata": {},
   "source": [
    "데이터 탐색은 개인적으로 연습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ac82a2-83bf-4224-9465-112cac22dc92",
   "metadata": {},
   "source": [
    "## 학습 및 테스트 데이터세트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1203d30-201d-4fec-8985-0f7f9a2e3d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/11 15:06:19 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5780 rows in the traiining set,\n",
      "and 1366 in the test set\n"
     ]
    }
   ],
   "source": [
    "trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42)\n",
    "print(f\"\"\"There are {trainDF.count()} rows in the traiining set,\n",
    "and {testDF.count()} in the test set\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f9528c-3743-4366-be58-347717525f4b",
   "metadata": {},
   "source": [
    "## 변환기를 사용하여 기능 준비\n",
    "침실 수를 기준으로 가격을 예측하는 선형 회귀 모델을 구축한다.\n",
    "\n",
    "스파크 머신러닝 알고리즘에서는 모든 입력 feature가 데이터 프레임의 단일 벡터 내에 포함되어야 한다. 따라서 데이터를 변환(transform)해야 한다. <br>\n",
    "\n",
    "스파크의 변환기는 데이터 프레임을 입력으로 받아들이고, 하나 이상의 열이 추가된 새 데이터 프레임을 반환한다. 그들은 데이터에서 학습하지 않지만 transform() 메서드를 사용하여 규칙 기반 변환을 적용한다. <br>\n",
    "\n",
    "모든 기능을 단일 벡터에 넣는 작업을 위해 VectorAssembler 변환기를 사용한다. 입력 열 목록을 가져와서 features라고 부를 추가 열이 있는 새 데이터 프레임을 만든다. 이러한 입력 열의 값을 단일 벡터로 결합한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b85f4a-5ee6-4106-8574-42bf45807b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+\n",
      "|bedrooms|features|price|\n",
      "+--------+--------+-----+\n",
      "|     1.0|   [1.0]|200.0|\n",
      "|     1.0|   [1.0]|130.0|\n",
      "|     1.0|   [1.0]| 95.0|\n",
      "|     1.0|   [1.0]|250.0|\n",
      "|     3.0|   [3.0]|250.0|\n",
      "|     1.0|   [1.0]|115.0|\n",
      "|     1.0|   [1.0]|105.0|\n",
      "|     1.0|   [1.0]| 86.0|\n",
      "|     1.0|   [1.0]|100.0|\n",
      "|     2.0|   [2.0]|220.0|\n",
      "+--------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vecAssembler = VectorAssembler(inputCols=['bedrooms'], outputCol='features')\n",
    "vecTrainDF = vecAssembler.transform(trainDF)\n",
    "vecTrainDF.select('bedrooms', 'features', 'price').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c804d8-ab5b-4dff-8e84-70bc40f84de9",
   "metadata": {},
   "source": [
    "## 추정기를 사용하여 모델 구축\n",
    "스파크에서 LinearRegression은 추정기의 한 유형이다. 데이터 프레임을 사용하고 모델을 반환한다.\n",
    "\n",
    "선형 회귀에 대한 입력 열(features)이 vectorAssembler의 출력이라는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "696721d7-628c-46ae-b486-49f172aaf46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/11 15:06:27 WARN Instrumentation: [052793c2] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/09/11 15:06:27 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/09/11 15:06:28 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol='features', labelCol='price')\n",
    "lrModel = lr.fit(vecTrainDF)\n",
    "# lr.fit()은 변환기인 LinearRegressionModel(lrModel)을 반환한다.\n",
    "# 즉, 추정기의 fit() 메서드의 출력은 변환기이다.\n",
    "# 추정기가 매개변수를 학습하면 변환기는 이러한 매개변수를 새 데이터 포인트에 적용하여 예측을 생성할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705775f7-ae46-45e9-b49f-d5470845a295",
   "metadata": {},
   "source": [
    "학습한 매개변수를 살펴본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba14f19c-7ba2-4b39-bd02-1caabee70b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price = 123.68*bedrooms + 47.51\n"
     ]
    }
   ],
   "source": [
    "m = round(lrModel.coefficients[0], 2)\n",
    "b = round(lrModel.intercept, 2)\n",
    "print(f\"\"\"price = {m}*bedrooms + {b}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca692661-eb20-46b8-9b9f-b5da63e0a82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([123.6757])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 참고!\n",
    "lrModel.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c28ea5-173d-4829-873b-e8a6fe432db1",
   "metadata": {},
   "source": [
    "## 파이프라인 생성\n",
    "모델을 테스트 세트에 적용하려면 훈련 세트와 동일한 방식으로 해당 데이터를 준비해야 한다. 데이터가 통과할 단계를 순서대로 지정하기만 하면 스파크가 알아서 처리를 한다.\n",
    "\n",
    "스파크에서 Pipelines는 추정기인 반면 PipelineModels(피팅된 파이프라인)는 변환기이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "085eac82-5bce-415e-a45d-d9fe7109982e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/11 15:06:30 WARN Instrumentation: [f931fcc5] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[vecAssembler, lr])\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4139ac5-7f2f-4675-abf6-5b6f16691f08",
   "metadata": {},
   "source": [
    "파이프라인 모델은 변환기이므로 테스트 데이터 세트에도 적용하는 것이 간단하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d213412e-c276-4db5-b329-f33511475082",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+------------------+\n",
      "|bedrooms|features| price|        prediction|\n",
      "+--------+--------+------+------------------+\n",
      "|     1.0|   [1.0]|  85.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  45.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  70.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 128.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 159.0|171.18598011578285|\n",
      "|     2.0|   [2.0]| 250.0|294.86172649777757|\n",
      "|     1.0|   [1.0]|  99.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  95.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 100.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|2010.0|171.18598011578285|\n",
      "+--------+--------+------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select('bedrooms', 'features', 'price', 'prediction').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d549f05-f834-44bd-973f-b128538cce77",
   "metadata": {},
   "source": [
    "범주형 feature를 사용하여 모델을 구축할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3097db9-72fc-409a-8916-634a92e0ab51",
   "metadata": {},
   "source": [
    "## 원-핫 인코딩\n",
    "MLlib 대부분의 머신러닝 모델은 벡터로 표시되는 숫자값을 입력으로 기대한다. 범주형 값을 숫자값으로 변환하기 위해 OHE(원-핫 인코딩)라는 기술을 사용할 수 있다. <br>\n",
    "\n",
    "300개의 범주형 값에 OHE를 적용하면 메모리/컴퓨팅 리소스 소비를 크게 증가시킬까? 스파크를 사용하면 그렇지 않다. 스파크는 대부분의 항목이 0일 때 내부적으로 SparseVector를 사용하므로 0값을 저장하는 공간을 낭비하지 않는다.\n",
    "\n",
    "SparseVector가 어떻게 작동하는지 예제로 살펴보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7599458-7f95-440c-9496-d5be4cb55f96",
   "metadata": {},
   "source": [
    "DenseVector(0, 0, 0, 7, 0, 2, 0, 0 ,0, 0) <br>\n",
    "SparseVector(10, [3,5], [7,2])\n",
    "\n",
    "DenseVector는 10개의 값을 포함하고 있으며 그 중 2개는 모두 0이다.<br>\n",
    "SparseVector를 생성하려면 벡터의 크기, 0이 아닌 요소의 인덱스 및 해당 인덱스의 해당 값을 추적해야 한다. 이 예에서 벡터의 크기는 10, 인덱스 3과 5에 0이 아닌 값이 두 개 있으며 해당 인덱스의 해당 값은 7과 2다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9adf93-d4ac-46eb-8d0c-1637ff4e75ef",
   "metadata": {},
   "source": [
    "스파크로 데이터를 원-핫 인코딩하는 몇 가지 방법이 있다.\n",
    "\n",
    "일반적인 접근 방식은 StringIndexer 및 OneHotEncoder를 사용하는 것이다. 첫 번째 단계는 StringIndexer 추정기를 적용하여 범주형 값을 범주 지수로 변환하는 것이다. 이러한 범주 인덱스는 레이블 빈도에 따라 정렬되므로 가장 빈번한 레이블은 인데스 0을 얻는다. 카테고리 인덱스를 만든 후에는 이를 OneHotEncoder에 대한 입력으로 전달할 수 있다. OneHotEncoder는 범주 인덱스 열을 이진 벡터열에 매핑한다.\n",
    "\n",
    "데이터 세트에서 문자열 유형의 모든 열은 범주형 특성으로 처리되지만 때로는 범주형으로 처리하거나 그 반대로 처리해야 하는 숫자 특성이 있을 수 있다. 어떤 열이 숫자이고 어떤 열이 범주인지 신중하게 식별해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d227c69-071f-4455-8b24-a2d7c1183bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes if dataType == 'string']\n",
    "indexOutputCols = [x + 'index' for x in categoricalCols]\n",
    "oheOutputCols = [x + 'OHE' for x in categoricalCols]\n",
    "\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols, outputCols=indexOutputCols,\n",
    "                              handleInvalid='skip')\n",
    "oheEncoder = OneHotEncoder(inputCols=indexOutputCols, outputCols=oheOutputCols)\n",
    "\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes if (dataType == 'double') \n",
    "              & (field != 'price')]\n",
    "assemblerInputs = oheOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d1554dd-f291-475f-aeae-2c010ced8db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('host_is_superhost', 'string'),\n",
       " ('cancellation_policy', 'string'),\n",
       " ('instant_bookable', 'string'),\n",
       " ('host_total_listings_count', 'double'),\n",
       " ('neighbourhood_cleansed', 'string'),\n",
       " ('latitude', 'double'),\n",
       " ('longitude', 'double'),\n",
       " ('property_type', 'string'),\n",
       " ('room_type', 'string'),\n",
       " ('accommodates', 'double'),\n",
       " ('bathrooms', 'double'),\n",
       " ('bedrooms', 'double'),\n",
       " ('beds', 'double'),\n",
       " ('bed_type', 'string'),\n",
       " ('minimum_nights', 'double'),\n",
       " ('number_of_reviews', 'double'),\n",
       " ('review_scores_rating', 'double'),\n",
       " ('review_scores_accuracy', 'double'),\n",
       " ('review_scores_cleanliness', 'double'),\n",
       " ('review_scores_checkin', 'double'),\n",
       " ('review_scores_communication', 'double'),\n",
       " ('review_scores_location', 'double'),\n",
       " ('review_scores_value', 'double'),\n",
       " ('price', 'double'),\n",
       " ('bedrooms_na', 'double'),\n",
       " ('bathrooms_na', 'double'),\n",
       " ('beds_na', 'double'),\n",
       " ('review_scores_rating_na', 'double'),\n",
       " ('review_scores_accuracy_na', 'double'),\n",
       " ('review_scores_cleanliness_na', 'double'),\n",
       " ('review_scores_checkin_na', 'double'),\n",
       " ('review_scores_communication_na', 'double'),\n",
       " ('review_scores_location_na', 'double'),\n",
       " ('review_scores_value_na', 'double')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 참고\n",
    "trainDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdcc5ed-bf58-45cb-9512-2c5f326e77f0",
   "metadata": {},
   "source": [
    "StringIndexer는 테스트 데이터 세트에는 나타나지만 훈련 데이터 세트에는 나타나지 않는 새로운 범주를 어떻게 처리하는가? <br>\n",
    "handleInvalid 매개변수에 처리 방법을 지정한다.\n",
    "1. skip: 잘못된 데이터가 있는 행 필터링\n",
    "2. error: 오류 발생\n",
    "3. 유지: 인덱스 numLabels의 특수 추가 버킷에 잘못된 데이터를 넣음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fedac2-948d-4bf6-8242-3abd6fc325e3",
   "metadata": {},
   "source": [
    "이 접근 방식의 한 가지 어려움은 StringIndexer에 범주형 피처로 처리해야 하는 피처를 명시적으로 알려야 한다는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48309dd3-540e-49f1-aef6-0d7c6e2f9b8c",
   "metadata": {},
   "source": [
    "또 다른 접근 방식은 RFormula를 사용하는 것이다. 이에 대한 구문은 R 프로그래밍 언어에서 영감을 받았다. <br>\n",
    "~,.,:,+ 및 -를 포함한 R 연산자의 제한된 하위 집합을 지원한다. 예를 들어 , 공식 = 'y ~ bedrooms + bathrooms'를 지정할 수 있다. 이는 bedroom과 bathroom만 주어지면 y를 예측한다는 것을 의미하고 'y ~ .'는 사용 가능한 모든 피처를 사용한다는 것을 의미한다. <br>\n",
    "RFormula는 자동으로 모든 문자열 열을 StriingIndex 및 원-핫 인코딩하고, 내부에서 VectorAssembler를 사용하여 이 모든 것을 단일 벡터로 결합한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87784ebc-7125-4f86-92f9-7b458d254ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "rFormula = RFormula(formula='price ~ .',\n",
    "                    featuresCol='features',\n",
    "                    labelCol='price',\n",
    "                    handleInvalid='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3389e6-3058-42bf-a59c-04238ccae466",
   "metadata": {},
   "source": [
    "RFormula의 단점은 원-핫 인코딩이 필요하지 않은 알고리즘에도 StringIndexer와 OneHotEncoder를 적용한다. 예를 들어 트리 기반 알고리즘은 범주형 피처에 대해 StringIndexer를 사용하기만 하면 범주형 변수를 직접 처리할 수 있다. 트리 기반 방법에 대해 범주형 피처를 원-핫 인코딩할 필요가 없으며 종종 모델을 악화시키는 경우가 많다.\n",
    "\n",
    "불행히도 피처 엔지니어링을 위한 만능 솔루션은 없으며 이상적인 접근 방식은 데이터 세트에 적용하려는 다운스트림 알고리즘과 밀접하게 관련되어 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c4c8d1-4a5f-4d7c-bd74-222e2273901a",
   "metadata": {},
   "source": [
    "모든 피처 준비 및 모델 구축을 파이프라인에 넣고 데이터 세트에 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "322fc536-cd38-44a8-b137-9fb3f0312a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/11 15:06:36 WARN Instrumentation: [cfac2a4a] regParam is zero, which might cause numerical instability and overfitting.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|price|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(98,[0,3,6,22,43,...| 85.0| 55.24365707389188|\n",
      "|(98,[0,3,6,22,43,...| 45.0|23.357685914717877|\n",
      "|(98,[0,3,6,22,43,...| 70.0|28.474464479034395|\n",
      "|(98,[0,3,6,12,42,...|128.0| -91.6079079594947|\n",
      "|(98,[0,3,6,12,43,...|159.0| 95.05688229945372|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(labelCol='price', featuresCol='features')\n",
    "pipeline = Pipeline(stages = [stringIndexer, oheEncoder, vecAssembler, lr])\n",
    "\n",
    "# 또는 RFormula 사용\n",
    "# pipeline = Pipeline(stages= [rFormula, lr])\n",
    "\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select('features', 'price', 'prediction').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca96f0fd-15c4-41ce-97ac-c3d7189a0dfd",
   "metadata": {},
   "source": [
    "전에 말했듯이 features열은 SparseVector로 표시된다. 원-핫 인코딩 후에는 98개의 피처가 있으며 그 다음에는 0이 아닌 인덱스와 값 자체가 있다. truncate=False를 show()에 전달하면 전체 출력을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7225e36-9436-4f58-87ba-b36d18b8000f",
   "metadata": {},
   "source": [
    "다음으로 전체 테스트 세트에서 모델이 얼마나 잘 수행되는지 수치적으로 평가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0135027-720b-4b80-88c4-0f23311d3b42",
   "metadata": {},
   "source": [
    "## 모델 평가\n",
    "RMSE를 사용하여 모델을 평가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6ecca08-8d6e-4b33-87ad-3a8a315587d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 220.6\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "regressionEvaluator = RegressionEvaluator(\n",
    "    predictionCol='prediction',\n",
    "    labelCol='price',\n",
    "    metricName='rmse')\n",
    "rmse = regressionEvaluator.evaluate(predDF)\n",
    "print(f\"RMSE is {rmse:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d73891-8168-4022-90f8-f1e69635e619",
   "metadata": {},
   "source": [
    "R2를 사용하여 모델을 평가한다.\n",
    "\n",
    "회귀 평가기를 재정의하는 대신 R2을 사용하도록 회귀 평가기를 변경하려면 setter 속성을 사용하여 메트릭 이름을 설정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2045e32-2e3d-42fd-81e1-bf72d1d517e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is 0.16043316698848087\n"
     ]
    }
   ],
   "source": [
    "r2 = regressionEvaluator.setMetricName('r2').evaluate(predDF)\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3753d6b6-3884-4204-a91a-878c6e1e7147",
   "metadata": {},
   "source": [
    "r2은 양수이지만 0에 매우 가깝다. 모델이 잘 수행되지 않는 이유 중 하나는 레이블인 price가 로그 정규 분포를 보이기 때문이다. 연습으로 로그스케일에서 가격을 예측하는 모델을 구축한 다음, 예측을 로그 스케일에서 벗어나 다시 지수화하여 모델을 평가한다. RMSE가 감소하고 R2이 증가하는 것을 확인해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa322d8-bfde-4d3c-a4fc-fb849939aaf3",
   "metadata": {},
   "source": [
    "## 모델 저장 및 로드\n",
    "모델을 나중에 재사용하기 위해 영구 저장소에 저장해보자(또는 클러스터가 다운되는 경우 모델을 다시 계산할 필요가 없음). API는 model.write().save(path)다. 선택적으로 overwrite() 명령을 제공하여 해당 경로에 포함된 데이터를 덮어쓸 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24230ed8-f152-46ca-855f-337bc881e8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinePath = '/tmp/lr-pipeline-model'\n",
    "pipelineModel.write().overwrite().save(pipelinePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42decd98-7d9a-4165-b47f-098c92a186b2",
   "metadata": {},
   "source": [
    "저장된 모델을 로드할 때 로드할 모델 유형을 다시 지정해야 한다(LinearRegressionModel 또는 LogisticRegression이었는지 등). 이러한 이유로 모든 모델에 대해 PipelineModel을 로드하고 모델에 대한 파일 경로만 변경하면 되도록 변환기/추정기를 항상 파이프라인에 배치하는 것이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42e2e556-4628-4edb-a68c-e879155e5391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "savedPipelineModel = PipelineModel.load(pipelinePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc608ec4-7ca8-4e15-9e27-a0de3a55f98b",
   "metadata": {},
   "source": [
    "로드한 후 이 모델을 새 데이터 포인트에 적용할 수 있다. 그러ㅓ나 스파크에는 '웜 스타트(warm start)'라는 개념이 없기 때문에 이 모델의 가중치를 새 모델 교육을 위한 초기화 매개변수로 사용할 수 없다. 데이터 세트가 약간 변형되면 전체 선형 회귀 모델을 처음부터 다시 훈련해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31dd5c8-67e8-4750-aa38-19b0931651cb",
   "metadata": {},
   "source": [
    "다른 모델이 데이터 세트에서 어떻게 수행되는지 살펴본다. 트리 기반 모델을 탐색하고, 모델 성능을 개선하기 위해 조정할 몇 가지 일반적인 하이퍼파라미터를 살펴본다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe7ee3-4652-444d-8eeb-8b6092a35bce",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeed16b-abc9-4812-a59f-4979990c89bc",
   "metadata": {},
   "source": [
    "### 트리 기반 모델\n",
    "트리 기반 방법은 자연스럽게 범주형 변수를 처리할 수 있다. spark.ml에서는 범주형 열을 StringIndexer에 전달하기만 하면 나머지는 의사결정나무에서 처리할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1a180a1-d00c-43ba-b13c-23c4c76f428c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/11 15:09:29 ERROR Instrumentation: java.lang.IllegalArgumentException: requirement failed: DecisionTree requires maxBins (= 32) to be at least as large as the number of values in each categorical feature, but categorical feature 3 has 36 values. Consider removing this and other categorical features with a large number of values, or add more training examples.\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:151)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n",
      "\tat org.apache.spark.ml.regression.DecisionTreeRegressor.$anonfun$train$1(DecisionTreeRegressor.scala:135)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:116)\n",
      "\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:46)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: DecisionTree requires maxBins (= 32) to be at least as large as the number of values in each categorical feature, but categorical feature 3 has 36 values. Consider removing this and other categorical features with a large number of values, or add more training examples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m stages \u001b[38;5;241m=\u001b[39m [stringIndexer, vecAssembler, dt]\n\u001b[1;32m     15\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages \u001b[38;5;241m=\u001b[39m stages)\n\u001b[0;32m---> 16\u001b[0m pipelineModel \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainDF\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: DecisionTree requires maxBins (= 32) to be at least as large as the number of values in each categorical feature, but categorical feature 3 has 36 values. Consider removing this and other categorical features with a large number of values, or add more training examples."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(labelCol='price')\n",
    "\n",
    "# 숫자 열만 필터링한다(price 제외).\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes\n",
    "               if ((dataType == 'double') & (field != 'price'))]\n",
    "\n",
    "# 위에서 정의한 StringIndexer의 출력과 숫자 열 결합\n",
    "assemblerInputs = indexOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol='features')\n",
    "\n",
    "# 단계를 파이프라인으로 결합\n",
    "stages = [stringIndexer, vecAssembler, dt]\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9c4e87-2c21-444f-b0d9-570071e28aa0",
   "metadata": {},
   "source": [
    "의도한 오류이다. maxBins 매개변수에 문제가 있음을 알 수 있다.\n",
    "\n",
    "이 매개변수는 무엇을 하는가? maxBins는 연속 특성이 이산화되거나 분할되는 빈의 수를 결정한다. 이 이산화 단계는 분산 교육을 수행하는데 중요하다. 모든 데이터와 모델이 단일 머신에 상주하기 때문에 사이킷런에는 maxBins 매개변수가 없다. 그러나 스파크에서 워커는 데이터의 모든 열을 갖고 있지만 행의 하위 집합만 있다. 따라서 분할할 피처와 값에 대해 통신할 때, 훈련 시간에 설정된 공통 이산화에서 얻은 동일한 분할값에 대해 모두 다루고 있는지 확인해야 한다.\n",
    "\n",
    "MLlib에서는 범주형 열의 이산화를 처리할 수 있을 만큼 maxBins가 충분히 커야한다. maxBins의 기본값은 32이고 36개의 고유한 값이 있는 범주형 열이 있었기 때문에 더 일직 오류가 발생했다. 64로 늘릴 수 있지만 시간이 크게 늘어난다. 대신 maxBins를 40으로 설정하고 파이프라인을 다시 훈련한다. 여기에서 setMaxBins() 메서드를 사용하여 결정 트리를 완전히 재정의하지 않고 수정하고 있음을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe666cc0-1be6-4562-94a7-d9f28464f2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dt.setMaxBins(40)\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0588e551-84de-4424-ae79-2087996b55f1",
   "metadata": {},
   "source": [
    "구현의 차이로 인해 사이킷런과 MLlib을 사용하여 모델을 빌드할 때 정확히 동일한 결과를 얻지 못하는 경우가 많다. 하지만 핵심은 왜 그들이 다른지 이해하고 필요한 방식으로 수행하도록 하기 위해 제어에 어떤 매개변수가 있는지 확인하는 것이다. 사이킷런에서 MLlib로 워크로드를 포팅하는 경우 spark.ml 및 사이킷런 문서에서 어떤 매개변수가 다른지 확인하고, 해당 매개변수를 조정하여 동일한 데이터에 대해 비교 가능한 결과를 얻을 것을 권장한다. 값이 충분히 가까워지면 사이킷런이 처리할 수 없는 더 큰 데이터 크기로 MLlib 모델을 확장할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c2258-032c-4197-8e85-4bc1b7fdb963",
   "metadata": {},
   "source": [
    "모델을 성공적으로 구축했으므로 의사결정나무에서 학습한 if-then-else 규칙을 추출할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bc98515-1af8-4ffe-b15e-b230b436a409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressionModel: uid=DecisionTreeRegressor_5f4b40c40403, depth=5, numNodes=47, numFeatures=33\n",
      "  If (feature 12 <= 2.5)\n",
      "   If (feature 12 <= 1.5)\n",
      "    If (feature 5 in {1.0,2.0})\n",
      "     If (feature 4 in {0.0,1.0,3.0,5.0,9.0,10.0,11.0,13.0,14.0,16.0,18.0,24.0})\n",
      "      If (feature 3 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0})\n",
      "       Predict: 104.23992784125075\n",
      "      Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0})\n",
      "       Predict: 250.7111111111111\n",
      "     Else (feature 4 not in {0.0,1.0,3.0,5.0,9.0,10.0,11.0,13.0,14.0,16.0,18.0,24.0})\n",
      "      If (feature 3 in {0.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,27.0,33.0,35.0})\n",
      "       Predict: 151.94179894179894\n",
      "      Else (feature 3 not in {0.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,27.0,33.0,35.0})\n",
      "       Predict: 245.8507462686567\n",
      "    Else (feature 5 not in {1.0,2.0})\n",
      "     If (feature 3 in {1.0,5.0,6.0,7.0,8.0,9.0,11.0,13.0,15.0,16.0,17.0,19.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 3 in {5.0,8.0,13.0,15.0,16.0,19.0,22.0,23.0,24.0,25.0,28.0,30.0,33.0})\n",
      "       Predict: 131.96658097686375\n",
      "      Else (feature 3 not in {5.0,8.0,13.0,15.0,16.0,19.0,22.0,23.0,24.0,25.0,28.0,30.0,33.0})\n",
      "       Predict: 164.19959266802445\n",
      "     Else (feature 3 not in {1.0,5.0,6.0,7.0,8.0,9.0,11.0,13.0,15.0,16.0,17.0,19.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 10 <= 6.5)\n",
      "       Predict: 205.5814889336016\n",
      "      Else (feature 10 > 6.5)\n",
      "       Predict: 841.6666666666666\n",
      "   Else (feature 12 > 1.5)\n",
      "    If (feature 13 <= 4.5)\n",
      "     If (feature 3 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,15.0,16.0,17.0,18.0,19.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,33.0,34.0})\n",
      "      If (feature 14 <= 26.5)\n",
      "       Predict: 290.8357933579336\n",
      "      Else (feature 14 > 26.5)\n",
      "       Predict: 214.04819277108433\n",
      "     Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,15.0,16.0,17.0,18.0,19.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,33.0,34.0})\n",
      "      If (feature 14 <= 3.5)\n",
      "       Predict: 741.64\n",
      "      Else (feature 14 > 3.5)\n",
      "       Predict: 309.03921568627453\n",
      "    Else (feature 13 > 4.5)\n",
      "     If (feature 15 <= 0.5)\n",
      "      If (feature 2 in {1.0})\n",
      "       Predict: 300.0\n",
      "      Else (feature 2 not in {1.0})\n",
      "       Predict: 10000.0\n",
      "     Else (feature 15 > 0.5)\n",
      "      If (feature 3 in {1.0,4.0,5.0,7.0,8.0,19.0})\n",
      "       Predict: 222.91666666666666\n",
      "      Else (feature 3 not in {1.0,4.0,5.0,7.0,8.0,19.0})\n",
      "       Predict: 398.0\n",
      "  Else (feature 12 > 2.5)\n",
      "   If (feature 1 in {0.0,1.0,2.0,3.0,4.0})\n",
      "    If (feature 12 <= 5.5)\n",
      "     If (feature 3 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,10.0,11.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,21.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 14 <= 7.5)\n",
      "       Predict: 493.3795620437956\n",
      "      Else (feature 14 > 7.5)\n",
      "       Predict: 296.76666666666665\n",
      "     Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,10.0,11.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,21.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 9 <= -122.411075)\n",
      "       Predict: 722.96875\n",
      "      Else (feature 9 > -122.411075)\n",
      "       Predict: 2399.4\n",
      "    Else (feature 12 > 5.5)\n",
      "     If (feature 4 in {0.0,1.0,5.0,7.0})\n",
      "      If (feature 3 in {0.0,3.0,6.0,25.0})\n",
      "       Predict: 609.5\n",
      "      Else (feature 3 not in {0.0,3.0,6.0,25.0})\n",
      "       Predict: 1715.0\n",
      "     Else (feature 4 not in {0.0,1.0,5.0,7.0})\n",
      "      Predict: 8000.0\n",
      "   Else (feature 1 not in {0.0,1.0,2.0,3.0,4.0})\n",
      "    Predict: 8000.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtModel = pipelineModel.stages[-1]\n",
    "print(dtModel.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea92cdfe-e295-47db-a41f-621780a3e17d",
   "metadata": {},
   "source": [
    "의사결정나무가 숫자 피처와 범주형 피처에서 분리되는 방식의 차이점에 주목하자. 숫자 기능의 경우 값이 임계값보다 작거나 같은지 확인하고 범주형 피처의 경우 값이 해당 세트에 있는지 여부를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b23a906-65bb-41d2-b516-af41afd84705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexerModel: uid=StringIndexer_75b6ee49d6da, handleInvalid=skip, numInputCols=7, numOutputCols=7,\n",
       " VectorAssembler_02fec71e23f3,\n",
       " DecisionTreeRegressionModel: uid=DecisionTreeRegressor_5f4b40c40403, depth=5, numNodes=47, numFeatures=33]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 참고\n",
    "pipelineModel.stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4199eba9-8781-44de-9e9d-fea9e5a03f05",
   "metadata": {},
   "source": [
    "기능 중요도 점수를 추출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fdee4d9-5c37-48be-a46c-c2b47da8197c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bedrooms</td>\n",
       "      <td>0.283406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cancellation_policyindex</td>\n",
       "      <td>0.167893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>instant_bookableindex</td>\n",
       "      <td>0.140081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>property_typeindex</td>\n",
       "      <td>0.128179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>number_of_reviews</td>\n",
       "      <td>0.126233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neighbourhood_cleansedindex</td>\n",
       "      <td>0.056200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>longitude</td>\n",
       "      <td>0.038810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>minimum_nights</td>\n",
       "      <td>0.029473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>beds</td>\n",
       "      <td>0.015218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>room_typeindex</td>\n",
       "      <td>0.010905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>accommodates</td>\n",
       "      <td>0.003603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>host_is_superhostindex</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bathrooms_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>beds_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>review_scores_rating_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>review_scores_accuracy_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>review_scores_cleanliness_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>review_scores_value</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>review_scores_checkin_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>review_scores_communication_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>review_scores_location_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bedrooms_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>review_scores_rating</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>review_scores_location</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>review_scores_communication</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>review_scores_checkin</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>review_scores_cleanliness</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>review_scores_accuracy</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bathrooms</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>latitude</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>host_total_listings_count</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bed_typeindex</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>review_scores_value_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          features  importance\n",
       "12                        bedrooms    0.283406\n",
       "1         cancellation_policyindex    0.167893\n",
       "2            instant_bookableindex    0.140081\n",
       "4               property_typeindex    0.128179\n",
       "15               number_of_reviews    0.126233\n",
       "3      neighbourhood_cleansedindex    0.056200\n",
       "9                        longitude    0.038810\n",
       "14                  minimum_nights    0.029473\n",
       "13                            beds    0.015218\n",
       "5                   room_typeindex    0.010905\n",
       "10                    accommodates    0.003603\n",
       "0           host_is_superhostindex    0.000000\n",
       "24                    bathrooms_na    0.000000\n",
       "25                         beds_na    0.000000\n",
       "26         review_scores_rating_na    0.000000\n",
       "27       review_scores_accuracy_na    0.000000\n",
       "28    review_scores_cleanliness_na    0.000000\n",
       "22             review_scores_value    0.000000\n",
       "29        review_scores_checkin_na    0.000000\n",
       "30  review_scores_communication_na    0.000000\n",
       "31       review_scores_location_na    0.000000\n",
       "23                     bedrooms_na    0.000000\n",
       "16            review_scores_rating    0.000000\n",
       "21          review_scores_location    0.000000\n",
       "20     review_scores_communication    0.000000\n",
       "19           review_scores_checkin    0.000000\n",
       "18       review_scores_cleanliness    0.000000\n",
       "17          review_scores_accuracy    0.000000\n",
       "11                       bathrooms    0.000000\n",
       "8                         latitude    0.000000\n",
       "7        host_total_listings_count    0.000000\n",
       "6                    bed_typeindex    0.000000\n",
       "32          review_scores_value_na    0.000000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "featureImp = pd.DataFrame(\n",
    "    list(zip(vecAssembler.getInputCols(), dtModel.featureImportances)),\n",
    "    columns = ['features', 'importance'])\n",
    "featureImp.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65213a8-72a1-47e2-89b0-57a25db5069e",
   "metadata": {},
   "source": [
    "더 나은 결과를 얻기 위해 다양한 모델을 결합하는 앙상블 접근 방식을 사용하여 이 모델을 개선하는 방법을 살펴본다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fc1e75-c349-4206-867c-bba7df07deb7",
   "metadata": {},
   "source": [
    "### 랜덤 포레스트\n",
    "설명 생략"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adb935bf-e8c9-4a86-8ecc-635cd21b1614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(labelCol='price', maxBins=40, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4adc88-8004-4521-8d8d-bd5dd1c5e2bb",
   "metadata": {},
   "source": [
    "랜덤 포레스트는 각 트리가 다른 트리와 독립적으로 구축될 수 있기 때문에 스파크를 사용한 분산 머신러닝의 힘을 진정으로 보여준다. 예를 들면 트리10을 구축하기 전에 투리 3을 구축할 필요가 없다. 또한 트리의 각 수준 내에서 작업을 병렬화하여 최적의 분할을 찾을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7859fc-69d4-4a2e-bc24-1fb301aaeaa7",
   "metadata": {},
   "source": [
    "### k-폴드 교차 검증\n",
    "설명 생략"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff59cec-43c8-41aa-9a51-0e77a2c4119b",
   "metadata": {},
   "source": [
    "스파크에서 하이퍼파라미터 검색을 수행하려면 다음 단계를 따른다.\n",
    "1. 평가할 추정기를 정의한다.\n",
    "2. ParamGridBuilder를 사용하여 변경하려는 하이퍼파라미터와 해당 값을 지정한다.\n",
    "3. 평가기(evaluator)를 정의하여 다양한 모델을 비교하는 데 사용할 메트릭을 지정한다.\n",
    "4. CrossValidator를 사용하여 다양한 모델 각각을 평가하는 교차 검증을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33b474f8-fb08-4546-a000-0653485f4531",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [stringIndexer, vecAssembler, rf])\n",
    "# 랜덤 포레스트 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73459e2d-349f-4611-9855-58b4e7785c17",
   "metadata": {},
   "source": [
    "ParamGridBuilder의 경우 maxDepth를 2, 4 또는 6으로 변경하고 numTrees(랜덤 포레스트의 트리 수)를 10 또는 100으로 변경한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "653723c0-e0c5-4f75-a433-5c3314d0ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [2, 4, 6])\n",
    "             .addGrid(rf.numTrees, [10, 100])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b0fa0e-13be-4a40-8fe4-11cd2409eeb6",
   "metadata": {},
   "source": [
    "각 모델을 평가하여 어떤 모델이 가장 성능이 좋은지 결정하는 방법을 정의한다. RegressionEvaluator를 사용하고 RMSE를 관심 메트릭으로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08eec8e6-ad2a-4c27-bdd1-70e9279df592",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol='price',\n",
    "                                predictionCol='prediction',\n",
    "                                metricName='rmse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d674f2-01cc-4a3d-8ad6-75a64f744125",
   "metadata": {},
   "source": [
    "estimator, evaluator, estimatorParamMaps를 받아들이는 CrossValidator를 사용하여 k-fold 교차 검증을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7dab3316-83e9-4e32-ab84-155a22c9a6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred                    \n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/home/minseok/.local/lib/python3.10/site-packages/pyspark/jars/spark-core_2.12-3.4.1.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/09/11 15:56:53 WARN DAGScheduler: Broadcasting large task binary with size 1321.2 KiB\n",
      "23/09/11 15:57:18 WARN DAGScheduler: Broadcasting large task binary with size 1154.8 KiB\n",
      "23/09/11 15:57:40 WARN DAGScheduler: Broadcasting large task binary with size 1195.6 KiB\n",
      "23/09/11 15:57:48 WARN DAGScheduler: Broadcasting large task binary with size 1222.0 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "                    evaluator=evaluator,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    numFolds=3,\n",
    "                    seed=42)\n",
    "cvModel = cv.fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3bbd8c-60c5-4f08-b386-970659850c39",
   "metadata": {},
   "source": [
    "스파크는 최적의 하이퍼파라미터 구성을 식별하면 전체 훈련 데이터 세트에 대해 모델을 재교육하므로 여기서 총 19개의 모델을 교육했다. 훈련된 중간 모델을 유지하려면 CrossValidator에서 collectSubModels=True를 설정할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9b9339-c00e-45f3-b68e-4ec9a987b8e8",
   "metadata": {},
   "source": [
    "교차 검증기의 결과를 검색하려면 avgMetrics를 살펴봐라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c352288-eb86-4a7f-a3d8-e012274623bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({Param(parent='RandomForestRegressor_a0d6edefae8a', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2,\n",
       "   Param(parent='RandomForestRegressor_a0d6edefae8a', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n",
       "  291.1822640924783),\n",
       " ({Param(parent='RandomForestRegressor_a0d6edefae8a', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2,\n",
       "   Param(parent='RandomForestRegressor_a0d6edefae8a', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n",
       "  286.7714750274078),\n",
       " ({Param(parent='RandomForestRegressor_a0d6edefae8a', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 4,\n",
       "   Param(parent='RandomForestRegressor_a0d6edefae8a', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n",
       "  287.6963245160818),\n",
       " ({Param(parent='RandomForestRegressor_a0d6edefae8a', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 4,\n",
       "   Param(parent='RandomForestRegressor_a0d6edefae8a', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n",
       "  279.9927057236079),\n",
       " ({Param(parent='RandomForestRegressor_a0d6edefae8a', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 6,\n",
       "   Param(parent='RandomForestRegressor_a0d6edefae8a', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n",
       "  294.34810870889305),\n",
       " ({Param(parent='RandomForestRegressor_a0d6edefae8a', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 6,\n",
       "   Param(parent='RandomForestRegressor_a0d6edefae8a', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n",
       "  275.39862704729984)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(cvModel.getEstimatorParamMaps(), cvModel.avgMetrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b766f9-5828-4137-aa25-b17e0cd366be",
   "metadata": {},
   "source": [
    "CrossValidator의 최상의 모델(RMSE가 가장 낮은 모델)이 maxDepth=6이고 numTrees=100임을 알 수 있다. 그러나 이것은 실행하는 데 오랜 시간이 걸렸다. 동일한 모델 성능을 유지하면서 모델 학습 시간을 줄이는 방법을 살펴본다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c25f034-19c3-4536-aad5-1aacf49fbb0e",
   "metadata": {},
   "source": [
    "### 파이프라인 최적화\n",
    "교차 검증에서 각 모델은 기술적으로 독립적이지만 spark.ml은 실제로 병렬이 아닌 순차적으로 모델 컬렉션을 훈련한다. 스파크 2.3에서는 이 문제를 해결하기 위해 parallelism 매개변수가 도입되었다. 이 매개변수는 병렬로 훈련할 모델의 수를 결정한다.\n",
    "\n",
    "paralleism값은 클러스터 리소스를 초과하지 않고 병렬 처리를 최대화하려면 신중하게 선택해야 하며 값이 더 크다고 항상 성능이 향상되는 것은 아니다. 일반적으로 대부분의 클러스터에는 최대 10이면 충분하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d4f268-448b-4c97-86a7-e50a2eef43a8",
   "metadata": {},
   "source": [
    "이 값을 4로 설정하고 더 빨리 훈련할 수 있는지 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63c51df1-defc-4fc7-b133-0ab66f0f5897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/11 16:11:47 WARN DAGScheduler: Broadcasting large task binary with size 1321.2 KiB\n",
      "23/09/11 16:12:04 WARN DAGScheduler: Broadcasting large task binary with size 1154.8 KiB\n",
      "23/09/11 16:12:18 WARN DAGScheduler: Broadcasting large task binary with size 1195.6 KiB\n",
      "23/09/11 16:12:24 WARN DAGScheduler: Broadcasting large task binary with size 1222.0 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cvModel = cv.setParallelism(4).fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea773526-197e-44da-a2c5-e2e9e84cd550",
   "metadata": {},
   "source": [
    "책에서 훈련 시간이 절반으로 줄었다.\n",
    "\n",
    "모델 훈련 속도를 높이는데 사용할 수 있는 또 다른 트릭이 있다. 파이프라인을 교차 검증기 내부에 배치하는 대신, 파이프라인 내부에 교차 검증기를 배치한다. 파이프라인의 모든 단계를 재평가하면서 변경되지 않더라도 동일한 StringIndexer 매핑을 반복해서 학습한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7469d782-d047-44c2-8ee4-656f343b3900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/11 16:22:28 WARN DAGScheduler: Broadcasting large task binary with size 1320.6 KiB\n",
      "23/09/11 16:22:40 WARN DAGScheduler: Broadcasting large task binary with size 1157.4 KiB\n",
      "23/09/11 16:22:42 WARN BlockManager: Block rdd_2532_0 already exists on this machine; not re-adding it\n",
      "23/09/11 16:22:52 WARN DAGScheduler: Broadcasting large task binary with size 1195.0 KiB\n",
      "23/09/11 16:23:00 WARN DAGScheduler: Broadcasting large task binary with size 1222.0 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cv = CrossValidator(estimator=rf,\n",
    "                    evaluator=evaluator,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    numFolds=3,\n",
    "                    parallelism=4,\n",
    "                    seed=42)\n",
    "\n",
    "pipeline = Pipeline(stages=[stringIndexer, vecAssembler, cv])\n",
    "# 알고리즘 대신 cv가 들어갔네\n",
    "# cv안에 모델이 인자로 들어가서일까\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f59404-1392-4452-b9d4-71b9607174dc",
   "metadata": {},
   "source": [
    "훈련 시간이 더 단축됐다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff69bf62-f19f-4844-893b-4b650d919463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
