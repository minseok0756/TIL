{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccacf546",
   "metadata": {},
   "source": [
    "# Spark DataFrame Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fe496e9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Using cached pyspark-3.4.1.tar.gz (310.8 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285432 sha256=3f0ce4247b3e3a690aad77de03805139e3cb49c7de182bb87659405e8f519b3e\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\e9\\b4\\d8\\38accc42606f6675165423e9f0236f8e825f6b6b6048d6743e\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "505a8c01",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark[ml,mllib,sql] in d:\\programdata\\anaconda3\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in d:\\programdata\\anaconda3\\lib\\site-packages (from pyspark[ml,mllib,sql]) (0.10.9.7)\n",
      "Requirement already satisfied: numpy>=1.15 in d:\\programdata\\anaconda3\\lib\\site-packages (from pyspark[ml,mllib,sql]) (1.24.3)\n",
      "Requirement already satisfied: pandas>=1.0.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from pyspark[ml,mllib,sql]) (1.5.3)\n",
      "Requirement already satisfied: pyarrow>=1.0.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from pyspark[ml,mllib,sql]) (11.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->pyspark[ml,mllib,sql]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->pyspark[ml,mllib,sql]) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.0.5->pyspark[ml,mllib,sql]) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark[sql,ml,mllib]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e4ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fde3ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5c02b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json 데이터 읽기\n",
    "df = spark.read.json('people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fadf5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 보기\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31f9acb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터프레임의 스키마 확인\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bb3820f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터프레임 열만 확인\n",
    "df.columns # pandas와 같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b79abe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, age: string, name: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터프레임의 통계적 요약(null값은 포함시키지 않음)\n",
    "df.describe()\n",
    "# DataFrame[summary: string, age: string, name: string]이 반환된다.\n",
    "# 문자열을 지닌 summary, age, name이라는 열을 불러왔다는 의미이다.\n",
    "# (출력값을 확인해보니 age, summay가 문자라는 의미인듯)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cd4201a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+\n",
      "|summary|               age|   name|\n",
      "+-------+------------------+-------+\n",
      "|  count|                 2|      3|\n",
      "|   mean|              24.5|   null|\n",
      "| stddev|7.7781745930520225|   null|\n",
      "|    min|                19|   Andy|\n",
      "|    max|                30|Michael|\n",
      "+-------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 값을 확인하려면 .show()\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2ff18e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 좋은 데이터나 특정 소스의 데이터가 아니라면 보통 스키마를 명확히 해야한다.\n",
    "# 어떤 열이 문자열이고 정수인지 등등을 스키마가 정확히 알고 있어야 한다.\n",
    "# 가장 먼저 타입 툴을 불러와야 한다.\n",
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType\n",
    "\n",
    "# 다음 단계로는 구조 필드의 리스트를 만들어줘야한다.\n",
    "# StructField의 매개 변수는 이름, 데이터 유형, 널 값이 될 수 있는지 여부 변수까지 3개가 있다.\n",
    "data_schema = [StructField('age',IntegerType(),True),\n",
    "# 이 필드와 관련된 열은 '나이'열이며, 나이는 정수로 표현되며, 널값을 가질 수 있다는 의미이다.\n",
    "# True가 아닌 False를 사용하면 null값을 넣었을 때 에러가 발생한다.\n",
    "              StructField('Name',StringType(),True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9854881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 구조로 final_struc 변수를 설정한다\n",
    "# fields에는 우리가 지정한 스키마를 대입한다.\n",
    "final_struc = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbb59323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 작업을 실행시키고, json 파일을 불러올 예정인데 스키마가 내가 원하는 최종 구조임을 명시한다.\n",
    "# 스키마를 매개 변수 인자로  넣어 최종 구조를 불러온다.\n",
    "df = spark.read.json('people.json', schema=final_struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2a17da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "# 코드를 실행시키면 우리가 정의한 스키마대로 출력되는것을 볼 수 있다.\n",
    "# (age의 데이터유형이 long에서 integer로 바뀐것을 확인할 수 있다.)\n",
    "# 만약에 특정 파일 포맷이 문자열로 읽히게 디폴트 설정이 되어 있다면\n",
    "# 이 방법을 통해 스키마 유형을 특정지을 수 있다.\n",
    "# 특히 난해한 데이터 유형을 사용할 때 힘든데 다행히도 스파크가 유형을 추론하는\n",
    "# 능력이 좋아지고 있다.\n",
    "# 우리 강의 자료는 csv파일이여서 스파크가 스키마 유추를 하는데 문제가 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc08f407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ee48a50",
   "metadata": {},
   "source": [
    "# Spark DataFrame Basic2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57f82f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json 데이터 읽기\n",
    "df = spark.read.json('people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60776b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'age'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce37edf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33cbad47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael'), Row(age=30, name='Andy')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)\n",
    "# [Row(age=None, name='Michael'), Row(age=30, name='Andy')]\n",
    "# 각 행으로 이루어진 리스트 반환 -> index사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3767bd43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=None, name='Michael')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "515830e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.head(2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad8ca713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 열 선택\n",
    "df.select('age')\n",
    "# 반환 타입이 데이터프레임이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0154218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e35c48fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, name: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여러 열 선택\n",
    "df.select(['age','name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39d860bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['age','name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c845fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+\n",
      "| age|   name|newage|\n",
      "+----+-------+------+\n",
      "|null|Michael|  null|\n",
      "|  30|   Andy|    30|\n",
      "|  19| Justin|    19|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 새 열 만들기\n",
    "# withColumn : 기본적으로 새 열을 추가하거나, 기존 열을 대체해서 새 데이터 집합을 반환\n",
    "df.withColumn('newage', df['age']).show()\n",
    "# 'newage' 컬럼이 추가된것을 볼 수 있다.\n",
    "# 'age'컬럼이 대체되는 것이 아님"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc4ffe5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|    age|   name|\n",
      "+-------+-------+\n",
      "|Michael|Michael|\n",
      "|   Andy|   Andy|\n",
      "| Justin| Justin|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 기존 컬럼값을 대체할 수 있다.\n",
    "df.withColumn('age', df['name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f5bc245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------+\n",
      "| age|   name|double_age|\n",
      "+----+-------+----------+\n",
      "|null|Michael|      null|\n",
      "|  30|   Andy|        60|\n",
      "|  19| Justin|        38|\n",
      "+----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 연산을 적용할 수도 있다.\n",
    "df.withColumn('double_age', df['age']*2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bd32621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 기억해야할건, 이 변화들이 원조 데이터 프레임에서 영구적이지 않다.\n",
    "df.show()\n",
    "# 원조 데이터프레임에는 변화가 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c69fcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|my_new_age|   name|\n",
      "+----------+-------+\n",
      "|      null|Michael|\n",
      "|        30|   Andy|\n",
      "|        19| Justin|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 컬럼 이름 변경\n",
    "df.withColumnRenamed('age','my_new_age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "202054f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pure sql에 대하여..\n",
    "# 직통으로 데이터프레임을 가지고 상호작용하는 쿼리 언어\n",
    "# 데이터프레임을 sql temporary view로 등록해야한다.\n",
    "# createOrReplaceTempView: view 생성. 이미 view가 존재하면 replace한다.\n",
    "df.createOrReplaceTempView('people')\n",
    "# people'은 view 이름을 설정하는듯하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d42c81d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 사용 방법\n",
    "results = spark.sql('SELECT * FROM people')\n",
    "results.show()\n",
    "# 출력: df 테이블이 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b12604e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_results = spark.sql('SELECT * FROM people WHERE age=30')\n",
    "new_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b332b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 장점: 몇 가지 기본 스파트 데이터프레임 작업을 잊어도 복잡한 작업을 빨리 할 수 있다.\n",
    "# 하지만 본 과정에서는 중점적으로 사용하지 않는다.\n",
    "# python과 pyspark의 특정 방법에 집중한다.\n",
    "# 하지만 sql 지식이 있다면 정말 유용한 도구가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec88e14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
