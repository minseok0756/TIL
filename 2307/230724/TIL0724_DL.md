## 딥러닝이란
- 딥러닝은 머신 러닝의 특정한 한 분야로서 연속된 층에서 점진적으로 의미 있는 표현을 배우는데 강점이 있으며, 데이터로부터 표현을 학습하는 새로운 방식
- 데이터로부터 모델을 만드는데 얼마나 많은 층을 사용했는지가 그 모델의 깊이가 된다.
- 딥러닝은 그냥 데이터로부터 표현을 학습하는 수학 모델일 뿐


## 딥러닝의 작동 원리
- 층이 입력 데이터를 처리하는 방식은 일련의 숫자로 이루어진 층의 가중치에 저장되어있음
- 기술적으로 말하면 어떤 층에서 일어나는 변환은 그 층의 가중치를 파라미터로 가지는 함수로 표현된다.

- 신경망의 출력을 제어하려면 출력이 기대하는 것보다 얼마나 벗어났는지 측정해야 한다.
- 이는 신경망의 손실 함수가 담당하는 일이다.
- 손실 함수를 목적함수 또는 비용함수라고도 부른다.
- 신경망이 한 샘플에 대해 얼마나 잘 예측했는지 측정하기 위해 손실 함수가 신경망의 예측과 진짜 타깃(신경망의 출력으로 기대하는 값)의 차이를 점수로 계산한다.(신경망의 예측과 타깃이 손실 함수의 인자, 점수가 손실함수의 함수값 느낌)
- ***딥러닝 방식은 이 점수를 피드백 신호로 사용하여 현재 샘플의 손실 점수가 감소되는 방향으로 가중치 값을 조금씩 수정하는 것임***
- 이런 수정 과정은 딥러닝의 핵심 알고리즘인 역전파 알고리즘을 구현한 옵티마이저가 담당함
- 초기에는 네트워크의 가중치가 랜덤한 값으로 할당되므로 랜덤한 변환을 연속적으로 수행한다.
- 자연스럽게 출력은 기대한 것과 멀어지고 손실 점수가 매우 높을 것이다.
- 하지만 네트워크가 모든 샘플을 처리하면서 가중치가 조금씩 올바른 방향으로 조정되고 손실 점수가 감소한다.
- 이를 훈련반복이라고 하며, 충분한 횟수만큼 반복하면(일반적으로 수천 개의 샘플에서 수십 번 반복하면) 손실 함수를 최소화하는 가중치 값을 산출한다.
- 최소한의 손실을 내는 네트워크가 타깃에 가능한 가장 가까운 출력을 만드는 모델이 된다.

## 딥러닝의 특징
- 딥러닝이 빠르게 확산된 주된 이유는 많은 문제에서 더 좋은 성능을 내고 있기 때문
- 뿐만 아니라 딥러닝은 머신 러닝에서 가장 중요한 단계인 특성 공학을 완전히 자동화하기 때문에 문제를 더 해결하기 쉽게 만들어줌
- 머신 러닝 방법은 처리하기 용이하게 사람이 초기 입력 데이터를 여러 방식으로 변환해야 한다.
- 데이터의 좋은 표현을 수동으로 만들어야 한다. 이를 특성 공학이라고 함
- 딥러닝을 사용하면 특성을 직접 찾는 대신 한 번에 모든 특성을 학습할 수 있다.
- 머신 러닝 워크플로를 매우 단순화시켜 주므로 고도의 다단계 작업 과정을 하나의 간단한 엔트-투-엔드 딥러닝 모델로 대체할 수 있다.

- 딥러닝의 변환 능력은 모델이 모든 표현 층을 순차적이 아니라 동시에 공동으로 학습하게 만든다.
- 이런 공동 특성 학습 능력 덕택에 모델이 내부 특성 하나를 조정할 때마다 이에 의존하는 다른 모든 특성이 사람이 개입하지 않아도 자동으로 변화에 적응하게 된다.

- 딥러닝이 데이터로부터 학습하는 방법에는 두 가지 중요한 특징이 있다.
- 층을 거치면서 점진적으로 더 복잡한 표현이 만들어진다는 것과 이런 점진적인 중간 표현이 공동으로 학습된다는 사실
- 각 층은 상위 층과 하위 층의 표현이 변함에 따라 함께 바뀐다.

## 신경망의 수학적 구성 요소
- 신경망의 핵심 구성 요소는 층이다. 데이터를 위한 필터로 생각할 수 있다.
- 더 구체적으로 층은 주어진 문제에 더 의미 있는 표현을 입력된 데이터로부터 추출한다.
- 대부분의 딥러닝은 간단한 층을 연결하여 구성되어 있고, 점진적으로 데이터를 정제하는 형태를 띤다.
- 딥러닝 모델은 데이터 정제 필터가 연속되어 있는 데이터 프로세싱을 위한 여과기와 같다.

- 신경망이 훈련 준비를 마치기 위해서 컴파일 단계에 포함되는 세 가지
    - 옵티마이저 - 성능을 향상시키기 위해 입력된 데이터를 기반으로 모델을 업데이트하는 매커니즘
    - 손실 함수 - 훈련 데이터에서 모델의 성능을 측정하는 방법. 모델이 옳은 방향으로 학습될 수 있도록 도와준다.
    - 훈련과 테스트 과정을 모니터링할 지표

- 훈련을 시작하기 전에 데이터를 모델에 맞는 크기로 바꾸고 모든 값을 0과 1 사이로 스케일 조정한다.

- 훈련 준비가 되었으면 케라스에서는 모델의 fit() 메서드를 호출하여 훈련 데이터에 모델을 학습시킨다.
- predict() - 예측
- evaluate() - 평가

## 신경망을 위한 데이터 표현

### 텐서
- 핵심적으로 텐서는 데이터를 위한 컨테이너이다. 일반적으로 수치형 데이터를 다룬다.
- 텐서는 임의의 차원 개수를 가지는 행렬의 일반화된 모습이다.
- 스칼라(랭크 - 0 텐서)
    - 축의 개수는 0개. 텐서의 축의 개수를 랭크라고도 부른다.
- 벡터(랭크 - 1 텐서)
    - 원소의 개수가 차원수임
    - 원소가 5인 벡터는 5차원 벡터, 5D 벡터라고 부름(5D 텐서와는 다른 개념. 축이 5개라는 의미)
- 행렬(랭크 -2 텐서)
- ...
    - 딥러닝에서는 보통 랭크 0 ~ 4까지의 텐서를 다룬다.
    - 동영상의 경우 랭크-5 텐서까지 다루기도 한다

- 핵심 속성
    - 축의 개수(랭크) - 넘파이, 텐서플로에서는 ndiim 속성
    - 크기(shape)
    - 데이터 타입 - 파이썬에서는 보통 dtype 속성

- 배치 데이터
    - 일반적으로 딥러닝에서 사용하는 모든 데이터 텐서의 첫 번째 축은 샘플 축이라고 함(샘플 차원이라고도 한다.)
    - 딥러닝 모델은 한 번에 전체 데이터셋을 처리하지 않는다.
    - 데이터를 작은 배치로 나눈다.
    - batch = train_image[128*n : 128*(n+1)] - n번째 배치 데이터
    - 이런 배치 데이터를 다룰 때 첫 번째 축을 배치 축 또는 배치 차원이라고 한다.

- 텐서의 실제 사례
    - 벡터 데이터 - (samples, features) 크기의 랭크-2 텐서. 각 샘플은 수치 속성으로 구성된 벡터이다.
        - 10만 명의 사림이 포함된 나이, 성별, 소득으로 구성된 인구 통계 데이터. (100000, 3)

    - 시계열 데이터 또는 시퀀스 데이터 - (samples, timesteps, features) 크기의 랭크-3 텐서. 각 샘플은 특성 벡터의 (길이가 timesteps인) 시퀀스입니다.
        - 벡터의 시퀀스는 랭크-2 텐서를 의미함
        - 1분마다 현재 주식 가격, 지난 1분 동안에 최고 가격과 최소 가격을 저장
        - 하루 거래는 (390,3) 크기의 행렬로 인코딩(하루 거래 시간이 390분이기 때문)
        - 250일치의 데이터는 (250, 390, 3) 크기의 랭크-3 텐서로 저장

    - 이미지 - (sample, height, widthm, channels) 또는 (smaple, channels, height, width) 크기의 랭크-4 텐서. 각 샘플은 픽셀의 2D 격자고 각 픽셀은 수치 값(채널)의 벡터 입니다.
        - 이미지는 전형적으로 높이, 너비, 컬러 채널의 3차원으로 이루어진다.
        - 흑백 이미지는 컬러 채널의 차원 크기는 1이다.
        - 두 가지 방식이 있는데 텐서플로에서는 채널 마지막 방식을 사용한다.
        - 케라스는 두 가지 방식 모두 지원

    - 동영상 - (samples, frames, height, width, channels) 또는 (samples, frames, channels, height, width) 크기의 랭크-5 텐서. 각 샘플은 이미지의 (길이가 frames인) 시퀀스입니다.
        - 하나의 비디오는 프레임의 연속이고 각 프레임은 하나의 컬러 이미지이다.
        - 프레임이 (height, width, color_depth)의 랭크-3 텐서로 저장될 수 있기 때문에 프레임의 연속은 (frames, height, width, color_depth)의 랭크-4 텐서로 저장될 수 있다.
        - 여러 비디오의 배치는 (samples, frames, height, width, color_depth)의 랭크-5 텐서로 저장될 수 있다.
        - 60초짜리 144x256 유튜브 비디오 클립을 초당 4프레임으로 샘플링하면 240프레임이 된다. 이런 비디오 클립을 4개 가진 배치는 (4, 240, 144, 256, 3) 크기의 텐서에 저장된다.

### 신경망의 톱니바퀴: 텐서 연산
- 원소별 덧셈, 뺄셈, 곱셈, 나눗셈
- 브로드캐스팅
- 텐서 곱셈(점곱)
    - 랭크-3 이상에서 점곱( ()는 shape을 의미 )
    - (a, b, c, d)·(d,) -> (a, b, c)
    - (a, b, c, d)·(d, e) -> (a, b, c, e)
- 텐서 크기 변환 - .reshape()

- 텐서 연산의 기하학적 해석
    - 선형 변환 - 임의의 행렬과 점곱하면 선형 변환이 수행된다. y = W·x
    - 아핀 변환(affine transform) 
        - (어떤 행렬과 점곱하여 얻는)선형 변환과 (벡터를 더해 얻는)이동의 조합이다. y = W·x + b(Dense층에서 수행되는 계산. 활성화 함수를 사용하지 않는 Dense층은 일종의 아핀 변환 층이다.)
        - 아핀 변환의 중요한 성질 하나는 여러 아핀 변환을 반복해서 적용해도 결국 하나의 아핀 변환이 된다는 것이다. 결국 활성화 함수 없이 Dense층으로만 구성된 다충 신경망은 하나의 Dense층과 같다. 이것이 relu 같은 활성화 함수가 필요한 이유이다.
        - 활성화 함수 덕분에 Dense 층을 중첩하여 매우 복잡하고 비선형적인 기하학적 변형을 구현하며 심층 신경망에 매우 풍부한 가설 공간을 제공할 수 있다.

- 딥러닝의 기하하적 해석
    - 신경망은 전체적으로 텐서 연산의 연결로 구성된 것이고, 모든 텐서 연산은 입력 데이터의 간단한 기하학적 변환임을 알았다.
    - 단순한 단계들이 길게 이어져 구현된 신경망을 고차원 공간에서 매우 복잡한 기하학적 변환을 하는 것으로 해석할 수 있다.
    - 색깔이 구별되는 색종이를 겹쳐서 뭉친 공을 만든다.
    - 이 종이 공이 입력데이터고 색종이는 분류 문제의 데이터 클래스이다.
    - 신경망이 해야 할 일은 종이 공을 펼쳐서 깔끔하게 분리시키는 변환을 찾는 것이다.
    - 손가락으로 종이 공을 조금씩 펼치는 것처럼 딥러닝을 사용하여 3D공간에서 간단한 변환들을 연결해서 이를 구현한다
    - 심층 신경망의 각 층은 데이터를 조금씩 풀어주는 변환을 적용하므로 이런 층을 깊게 쌓으면 아주 복잡한 분해 과정을 처리할 수 있다.

### 신경망의 엔진: 그레이디언트 기반 최적화
- output = relu(dot(W, input) + b)
    - W, b를 가중치 또는 훈련되는 파라미터라고 한다.(각각 커널과 편향(bias)라고 부르기도 한다.)
    - 가중치에는 훈련 데이터를 모델에 노출시켜서 학습된 정보가 담겨있다.
    - 처음에는 가중치 행렬이 작은 난수로 채워져 있다.(무작위 초기화 단계)
    - 피드백 신호에 기초하여 가중치가 점진적으로 조정된다.
    - 훈련 반복 루프

        1. 훈련 샘플 x와 이에 상응하는 타깃 y_true의 배치를 추출한다.
        2. x를 사용하여 모델을 실행하고(정방향 패스 단계) 예측 y_pred를 구한다.
        3. y_pred와 y_true의 차이를 측정하여 이 배치에 대한 모델의 손실을 계산하다.
        4. 배치에 대한 손실이 조금 감소되도록 모델의 모든 가중치를 업데이트한다.

    - 4번에서 가중치를 업데이트하는 방법 - 경사 하강법(gradient descent)
        - gradient 수학 연산을 사용하면 모델 가중치를 여러 방향으로 이동했을 때 손실이 얼마나 변하는지 설명할 수 있다.
        - 이를 사용하여 손실이 감소하는 방향으로 가중치를 (한 번에 하나씩이 아니라 한 번의 업데이트로 전체 가중치를 동시에) 이동시킬 수 있다.
            1. 훈련 샘플 x와 이에 상응하는 타깃 y_true의 배치를 추출한다.
            2. x를 사용하여 모델을 실행하고(정방향 패스 단계) 예측 y_pred를 구한다.
            3. y_pred와 y_true의 차이를 측정하여 이 배치에 대한 모델의 손실을 계산하다.
            4. 모델의 파라미터에 대한 손실 함수의 그레이디언트를 계산합니다.(이를 역방향 패스라고 부른다.)
            5. 그레이디언트의 반대 방향으로 파라미터를 조금 이동시킵니다. 예를 들어 W -= learning_rate * gradient 처럼하면 배치에 대한 손실이 조금 감소할 것이다. 학습률은 경사하강법 과정의 속도를 조절하는 스칼라 값이다.
            - 이를 미니 배치 확률적 경사 하강법이라고 한다.(SGD)
            - 확률적이란 의미는 각 배치 데이터가 무작위로 선택된 것임을 의미한다.
            - 학습률이 너무 작으면 곡선을 따라 내려가는데 너무 많은 반복이 필요하고 지역 최솟값에 갇힐 수 있다. 너무 크면 손실 함수 곡선에서 완전히 임의의 위치로 이동시킬 수 있다.
        
        - SGD 변종
            - 모멘텀을 사용한 SGD, Adagrad, RMSProp 등
            - 최적화 방법 또는 옵티마이저라고 부른다.
            - 모멘텀은 SGD에 있는 문제점인 수렴속도와 지역 최솟값을 해결한다.
            - 손실 곡선 위로 작은 공을 굴리는 것으로 생각하자
            - 모멘텀이 충분하면 공이 골짜기에 갇히지 않고 전역 최솟값에 도달할 것이다.
            - 모멘텀은 현재 기울기 값(현재 가속도)뿐만 아니라 (과거의 가속도로 인한) 현재 속도를 함께 고려하여 각 단계에서 공을 움직인다.
            - 실전에 적용할 때는 현재 그레이디언트 값뿐만 아니라 이전에 업데이트한 파라미터에 기초하여 파라미터 W를 업데이트한다.

### 도함수 연결: 역전파 알고리즘
- 그레이디언트를 쉽게 구할 수 있을까? 어떻게 구할 수 있을까? => 역전파 알고리즘
- 역전파는 (덧셈, 렐루, 텐서 곱셈 같은) 간단한 연산의 도함수를 사용해서 이런 기초적인 연산을 조합한 복잡한 연산의 그레이디언트를 쉽게 계산하는 방법이다. -> 연쇄 법칙 이용
- 역전파는 최종 손실 값에서 시작하여 아래층에서 맨 위층까지 거꾸로 거슬러 올라가 각 파라미터가 손실 값에 기여한 정도를 계산(연쇄법칙을 사용하기 때문에 자연스러운 논리)
- ***요즘에는 텐서플로와 같이 자동 미분이 가능한 최신 프레임워크를 사용하기 때문에 수동으로 역전파를 구현할 필요가 전혀 없다.***

- 텐서플로의 강력한 자동 미분 기능을 활용할 수 있는 API는 GradientTape이다.
- 파이썬의 with문과 함께 사용하여 해당 코드 블록 안의 모든 텐서 연산을 계산 그래프 형태로 기록한다.
- 그 다음 이 그래프를 사용해서 (tf.Variable 클래스의 인스턴스) 변수 또는 변수 집합에 대한 어떤 출력의 그레이디언트도 계산할 수 있다.
- tf.Variable은 변경 가능한 상태를 담기 위한 특별한 종류의 텐서이다.
- 예를 들어 신경망의 가중치는 항상 tf.Variable의 인스턴스이다.




























































































